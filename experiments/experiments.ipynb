{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_preprocessing_toolkit/preprocessor.py\n",
    "\n",
    "import re\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        remove_stopwords=True,\n",
    "        lemmatize=True,\n",
    "        lowercase=True,\n",
    "        remove_punctuation=True,\n",
    "    ):\n",
    "        self.remove_stopwords = remove_stopwords\n",
    "        self.lemmatize = lemmatize\n",
    "        self.lowercase = lowercase\n",
    "        self.remove_punctuation = remove_punctuation\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.stop_words = set(stopwords.words(\"english\"))\n",
    "\n",
    "    def preprocess(self, text):\n",
    "        if self.lowercase:\n",
    "            text = text.lower()\n",
    "        if self.remove_punctuation:\n",
    "            text = self._remove_punctuation(text)\n",
    "        tokens = word_tokenize(text)\n",
    "        if self.remove_stopwords:\n",
    "            tokens = self._remove_stopwords(tokens)\n",
    "        if self.lemmatize:\n",
    "            tokens = self._lemmatize(tokens)\n",
    "        return \" \".join(tokens)\n",
    "\n",
    "    def _remove_punctuation(self, text):\n",
    "        return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    def _remove_stopwords(self, tokens):\n",
    "        return [word for word in tokens if word not in self.stop_words]\n",
    "\n",
    "    def _lemmatize(self, tokens):\n",
    "        return [self.lemmatizer.lemmatize(word) for word in tokens]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "    \"Text preprocessing is a crucial step in natural language processing (NLP) and machine learning workflows, enabling more accurate and efficient text analysis. Raw text data is often messy and unstructured, containing inconsistencies, irrelevant characters, stop words, and varied forms of words. Through text preprocessing, this data can be cleaned, standardized, and transformed into a format better suited for algorithms to understand and analyze. Key steps include tokenization, which breaks down text into individual words or tokens; lemmatization, which converts words to their base or root forms; and removing stop words, which are common words (like \"\n",
    "    'the,\" \"and,\" \"is'\n",
    "    \") that don’t add substantial meaning to the text. Additionally, text normalization techniques—such as lowercasing and punctuation removal—help in reducing dimensionality and ensuring consistency. Effective text preprocessing not only improves model performance by reducing noise and redundancy but also enhances the interpretability of text-based insights in applications like sentiment analysis, document classification, and chatbot responses.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = TextPreprocessor(remove_stopwords=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Main Code**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Gaurav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Gaurav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Gaurav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# text_preprocessing_toolkit/preprocessing.py\n",
    "\n",
    "\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")  # Optional, improves WordNet lemmatization\n",
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.stopwords = set(stopwords.words(\"english\"))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.spell = SpellChecker()\n",
    "        self.ps = PorterStemmer()\n",
    "        self.wordnet_map = {\n",
    "            \"N\": wordnet.NOUN,\n",
    "            \"V\": wordnet.VERB,\n",
    "            \"J\": wordnet.ADJ,\n",
    "            \"R\": wordnet.ADV,\n",
    "        }\n",
    "\n",
    "    def remove_punctuation(self, text):\n",
    "        return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        return \" \".join(\n",
    "            [word for word in str(text).split() if word not in self.stopwords]\n",
    "        )\n",
    "\n",
    "    def get_frequent_words(self, texts, top_n=10):\n",
    "        word_counts = Counter()\n",
    "        for text in texts:\n",
    "            for word in text.split():\n",
    "                word_counts[word] += 1\n",
    "        return set([word for word, count in word_counts.most_common(top_n)])\n",
    "\n",
    "    def remove_frequent_words(self, text, frequent_words):\n",
    "        return \" \".join(\n",
    "            [word for word in str(text).split() if word not in frequent_words]\n",
    "        )\n",
    "\n",
    "    def get_rare_words(self, texts, bottom_n=10):\n",
    "        word_counts = Counter()\n",
    "        for text in texts:\n",
    "            for word in text.split():\n",
    "                word_counts[word] += 1\n",
    "        return set(\n",
    "            word for word, count in word_counts.most_common()[: -bottom_n - 1 : -1]\n",
    "        )\n",
    "\n",
    "    def remove_rare_words(self, text, rare_words):\n",
    "        return \" \".join([word for word in str(text).split() if word not in rare_words])\n",
    "\n",
    "    def remove_special_characters(self, text):\n",
    "        text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        return text\n",
    "\n",
    "    def stem_text(self, text):\n",
    "        return \" \".join([self.ps.stem(word) for word in text.split()])\n",
    "\n",
    "    def lemmatize_text(self, text):\n",
    "        pos_text = pos_tag(text.split())\n",
    "        return \" \".join(\n",
    "            [\n",
    "                self.lemmatizer.lemmatize(\n",
    "                    word, self.wordnet_map.get(pos[0], wordnet.NOUN)\n",
    "                )\n",
    "                for word, pos in pos_text\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # def lemmatize_text(self, text):\n",
    "    #     pos_text = pos_tag(text.split())\n",
    "    #     return \" \".join([self.lemmatizer.lemmatize(word, self.wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_text])\n",
    "\n",
    "    def remove_url(self, text):\n",
    "        return re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
    "\n",
    "    def remove_html_tags(self, text):\n",
    "        return re.sub(r\"<[^>]+>\", \"\", text)\n",
    "\n",
    "    def correct_spellings(self, text):\n",
    "        corrected_text = []\n",
    "        misspelled_words = self.spell.unknown(text.split())\n",
    "        for word in text.split():\n",
    "            if word in misspelled_words:\n",
    "                corrected_word = self.spell.correction(word)\n",
    "                corrected_text.append(corrected_word)\n",
    "            else:\n",
    "                corrected_text.append(word)\n",
    "        return \" \".join(corrected_text)\n",
    "\n",
    "    def lowercase(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    def preprocess(self, text, steps=None):\n",
    "        if steps is None:\n",
    "            steps = [\n",
    "                \"lowercase\",\n",
    "                \"remove_punctuation\",\n",
    "                \"remove_stopwords\",\n",
    "                \"remove_special_characters\",\n",
    "                \"remove_url\",\n",
    "                \"remove_html_tags\",\n",
    "                \"correct_spellings\",\n",
    "                \"lemmatize_text\",\n",
    "            ]\n",
    "        for step in steps:\n",
    "            text = getattr(self, step)(text)\n",
    "        return text\n",
    "\n",
    "    def head(self, texts, n=5):\n",
    "        \"\"\"\n",
    "        Display the first few entries of the dataset for quick visualization.\n",
    "\n",
    "        Parameters:\n",
    "        texts (list or pd.Series): The dataset or list of text entries to display.\n",
    "        n (int): The number of rows to display. Default is 5.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        if isinstance(texts, (list, pd.Series)):\n",
    "            data = pd.DataFrame({\"Text\": texts[:n]})\n",
    "            print(data)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"The input should be a list or pandas Series of text entries.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing punctuation: Hello world This is a test text with HTML html tags and a URL httpsexamplecom\n",
      "Lowercase text: hello, world! this is a test text with html <html> tags and a url: https://example.com.\n",
      "After removing stopwords: Hello, world! This test text HTML <html> tags URL: https://example.com.\n",
      "After removing URLs: Hello, world! This is a test text with HTML <html> tags and a URL: \n",
      "After removing HTML tags: Hello, world! This is a test text with HTML  tags and a URL: https://example.com.\n",
      "After spell correction: This is a test text for spelling correction\n",
      "After lemmatization: run jump play well\n",
      "After stemming: run jump play better\n"
     ]
    }
   ],
   "source": [
    "processor = TextPreprocessor()\n",
    "\n",
    "# Sample text\n",
    "text = \"Hello, world! This is a test text with HTML <html> tags and a URL: https://example.com.\"\n",
    "\n",
    "# Removing punctuation\n",
    "no_punctuation_text = processor.remove_punctuation(text)\n",
    "print(\"After removing punctuation:\", no_punctuation_text)\n",
    "\n",
    "# Lowercasing text\n",
    "lowercase_text = processor.lowercase(text)\n",
    "print(\"Lowercase text:\", lowercase_text)\n",
    "\n",
    "# Removing stopwords\n",
    "no_stopwords_text = processor.remove_stopwords(text)\n",
    "print(\"After removing stopwords:\", no_stopwords_text)\n",
    "\n",
    "# Removing URLs\n",
    "no_urls_text = processor.remove_url(text)\n",
    "print(\"After removing URLs:\", no_urls_text)\n",
    "\n",
    "# Removing HTML tags\n",
    "no_html_text = processor.remove_html_tags(text)\n",
    "print(\"After removing HTML tags:\", no_html_text)\n",
    "\n",
    "# Correcting spellings\n",
    "corrected_text = processor.correct_spellings(\"This is a tst txt for spellng corection\")\n",
    "print(\"After spell correction:\", corrected_text)\n",
    "\n",
    "# Lemmatizing text\n",
    "lemmatized_text = processor.lemmatize_text(\"running jumped plays better\")\n",
    "print(\"After lemmatization:\", lemmatized_text)\n",
    "\n",
    "\n",
    "# Stemming text\n",
    "stemmed_text = processor.stem_text(\"running jumped plays better\")\n",
    "print(\"After stemming:\", stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Text\n",
      "0          This is the first entry.\n",
      "1         Another example sentence.\n",
      "2  Text preprocessing is essential.\n",
      "                               Text\n",
      "0          This is the first entry.\n",
      "1         Another example sentence.\n",
      "2  Text preprocessing is essential.\n",
      "                               Text  Length\n",
      "0          This is the first entry.      24\n",
      "1         Another example sentence.      25\n",
      "2  Text preprocessing is essential.      32\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        # Initialize the attributes for text preprocessing\n",
    "        self.stopwords = set(stopwords.words(\"english\"))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.spell = SpellChecker()\n",
    "        self.ps = PorterStemmer()\n",
    "        self.wordnet_map = {\n",
    "            \"N\": wordnet.NOUN,\n",
    "            \"V\": wordnet.VERB,\n",
    "            \"J\": wordnet.ADJ,\n",
    "            \"R\": wordnet.ADV,\n",
    "        }\n",
    "\n",
    "    # Existing preprocessing functions\n",
    "    # ...\n",
    "\n",
    "    def head(self, data, n=5):\n",
    "        \"\"\"\n",
    "        Display the first few entries of the data for quick visualization.\n",
    "\n",
    "        Parameters:\n",
    "        data (list, pd.Series, pd.DataFrame): The data to display.\n",
    "        n (int): The number of rows to display. Default is 5.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        if isinstance(data, list):\n",
    "            data = pd.DataFrame({\"Text\": data})\n",
    "\n",
    "        if isinstance(data, pd.Series):\n",
    "            data = data.to_frame(\"Text\")\n",
    "\n",
    "        if isinstance(data, pd.DataFrame):\n",
    "            print(data.head(n))\n",
    "        else:\n",
    "            raise ValueError(\"The input should be a list, pandas Series, or DataFrame.\")\n",
    "\n",
    "\n",
    "# Sample list of text entries\n",
    "sample_texts = [\n",
    "    \"This is the first entry.\",\n",
    "    \"Another example sentence.\",\n",
    "    \"Text preprocessing is essential.\",\n",
    "    \"This toolkit is very useful.\",\n",
    "    \"It provides various functions for NLP.\",\n",
    "]\n",
    "\n",
    "# Sample Series of text entries\n",
    "sample_series = pd.Series(sample_texts)\n",
    "\n",
    "# Sample DataFrame\n",
    "sample_df = pd.DataFrame(\n",
    "    {\"Text\": sample_texts, \"Length\": [len(text) for text in sample_texts]}\n",
    ")\n",
    "\n",
    "# Initialize the TextPreprocessor instance\n",
    "processor = TextPreprocessor()\n",
    "\n",
    "# Using head() on a list\n",
    "processor.head(sample_texts, n=3)\n",
    "\n",
    "# Using head() on a Series\n",
    "processor.head(sample_series, n=3)\n",
    "\n",
    "# Using head() on a DataFrame\n",
    "processor.head(sample_df, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Gaurav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Gaurav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Gaurav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                               Text\n",
      "0          This is the first entry.\n",
      "1         Another example sentence.\n",
      "2  Text preprocessing is essential.\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from spellchecker import SpellChecker\n",
    "import nltk\n",
    "\n",
    "# Download necessary NLTK data files\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")  # Optional, improves WordNet lemmatization\n",
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.stopwords = set(stopwords.words(\"english\"))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.spell = SpellChecker()\n",
    "        self.ps = PorterStemmer()\n",
    "        self.wordnet_map = {\n",
    "            \"N\": wordnet.NOUN,\n",
    "            \"V\": wordnet.VERB,\n",
    "            \"J\": wordnet.ADJ,\n",
    "            \"R\": wordnet.ADV,\n",
    "        }\n",
    "\n",
    "    # Other preprocessing methods...\n",
    "\n",
    "    def head(self, texts, n=5):\n",
    "        \"\"\"\n",
    "        Display the first few entries of the dataset for quick visualization.\n",
    "\n",
    "        Parameters:\n",
    "        texts (list or pd.Series): The dataset or list of text entries to display.\n",
    "        n (int): The number of rows to display. Default is 5.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        if isinstance(texts, (list, pd.Series)):\n",
    "            data = pd.DataFrame({\"Text\": texts[:n]})\n",
    "            print(data)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"The input should be a list or pandas Series of text entries.\"\n",
    "            )\n",
    "\n",
    "\n",
    "processor = TextPreprocessor()\n",
    "processor.head(sample_texts, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'TextPreprocessor' object has no attribute 'lemmatize_text'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m lemmatized_text \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize_text\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunning jumped plays better\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter lemmatization:\u001b[39m\u001b[38;5;124m\"\u001b[39m, lemmatized_text)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'TextPreprocessor' object has no attribute 'lemmatize_text'"
     ]
    }
   ],
   "source": [
    "lemmatized_text = processor.lemmatize_text(\"running jumped plays better\")\n",
    "print(\"After lemmatization:\", lemmatized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing punctuation: Hello world This is a test text with HTML html tags and a URL httpsexamplecom\n",
      "Lowercase text: hello, world! this is a test text with html <html> tags and a url: https://example.com.\n",
      "After removing stopwords: Hello, world! This test text HTML <html> tags URL: https://example.com.\n",
      "After removing URLs: Hello, world! This is a test text with HTML <html> tags and a URL: \n",
      "After removing HTML tags: Hello, world! This is a test text with HTML  tags and a URL: https://example.com.\n",
      "After spell correction: This is a test text for spelling correction\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Gaurav/nltk_data'\n    - 'c:\\\\Users\\\\Gaurav\\\\OneDrive\\\\Desktop\\\\TPT\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\Gaurav\\\\OneDrive\\\\Desktop\\\\TPT\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Gaurav\\\\OneDrive\\\\Desktop\\\\TPT\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Gaurav\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[66], line 31\u001b[0m\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter spell correction:\u001b[39m\u001b[38;5;124m\"\u001b[39m, corrected_text)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;66;03m# Lemmatizing text\u001b[39;00m\n\u001b[1;32m---> 31\u001b[0m lemmatized_text \u001b[38;5;241m=\u001b[39m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlemmatize_text\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrunning jumped plays better\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAfter lemmatization:\u001b[39m\u001b[38;5;124m\"\u001b[39m, lemmatized_text)\n\u001b[0;32m     36\u001b[0m \u001b[38;5;66;03m# Stemming text\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[63], line 66\u001b[0m, in \u001b[0;36mTextPreprocessor.lemmatize_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     65\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize_text\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m---> 66\u001b[0m     pos_text \u001b[38;5;241m=\u001b[39m \u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwordnet_map\u001b[38;5;241m.\u001b[39mget(pos[\u001b[38;5;241m0\u001b[39m], wordnet\u001b[38;5;241m.\u001b[39mNOUN)) \u001b[38;5;28;01mfor\u001b[39;00m word, pos \u001b[38;5;129;01min\u001b[39;00m pos_text])\n",
      "File \u001b[1;32mc:\\Users\\Gaurav\\OneDrive\\Desktop\\TPT\\.venv\\lib\\site-packages\\nltk\\tag\\__init__.py:168\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_tag\u001b[39m(tokens, tagset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[1;32mc:\\Users\\Gaurav\\OneDrive\\Desktop\\TPT\\.venv\\lib\\site-packages\\nltk\\tag\\__init__.py:110\u001b[0m, in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    108\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m PerceptronTagger(lang\u001b[38;5;241m=\u001b[39mlang)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "File \u001b[1;32mc:\\Users\\Gaurav\\OneDrive\\Desktop\\TPT\\.venv\\lib\\site-packages\\nltk\\tag\\perceptron.py:183\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[1;34m(self, load, lang)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Gaurav\\OneDrive\\Desktop\\TPT\\.venv\\lib\\site-packages\\nltk\\tag\\perceptron.py:273\u001b[0m, in \u001b[0;36mPerceptronTagger.load_from_json\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;66;03m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[1;32m--> 273\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaggers/averaged_perceptron_tagger_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(loc \u001b[38;5;241m+\u001b[39m TAGGER_JSONS[lang][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(fin)\n",
      "File \u001b[1;32mc:\\Users\\Gaurav\\OneDrive\\Desktop\\TPT\\.venv\\lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Gaurav/nltk_data'\n    - 'c:\\\\Users\\\\Gaurav\\\\OneDrive\\\\Desktop\\\\TPT\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\Gaurav\\\\OneDrive\\\\Desktop\\\\TPT\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Gaurav\\\\OneDrive\\\\Desktop\\\\TPT\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Gaurav\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "processor = TextPreprocessor()\n",
    "\n",
    "# Sample text\n",
    "text = \"Hello, world! This is a test text with HTML <html> tags and a URL: https://example.com.\"\n",
    "\n",
    "# Removing punctuation\n",
    "no_punctuation_text = processor.remove_punctuation(text)\n",
    "print(\"After removing punctuation:\", no_punctuation_text)\n",
    "\n",
    "# Lowercasing text\n",
    "lowercase_text = processor.lowercase(text)\n",
    "print(\"Lowercase text:\", lowercase_text)\n",
    "\n",
    "# Removing stopwords\n",
    "no_stopwords_text = processor.remove_stopwords(text)\n",
    "print(\"After removing stopwords:\", no_stopwords_text)\n",
    "\n",
    "# Removing URLs\n",
    "no_urls_text = processor.remove_url(text)\n",
    "print(\"After removing URLs:\", no_urls_text)\n",
    "\n",
    "# Removing HTML tags\n",
    "no_html_text = processor.remove_html_tags(text)\n",
    "print(\"After removing HTML tags:\", no_html_text)\n",
    "\n",
    "# Correcting spellings\n",
    "corrected_text = processor.correct_spellings(\"This is a tst txt for spellng corection\")\n",
    "print(\"After spell correction:\", corrected_text)\n",
    "\n",
    "# Lemmatizing text\n",
    "lemmatized_text = processor.lemmatize_text(\"running jumped plays better\")\n",
    "print(\"After lemmatization:\", lemmatized_text)\n",
    "\n",
    "\n",
    "# Stemming text\n",
    "stemmed_text = processor.stem_text(\"running jumped plays better\")\n",
    "print(\"After stemming:\", stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def head(self, texts, n=5):\n",
    "    \"\"\"\n",
    "    Display a summary of the first few entries of the dataset for quick visualization.\n",
    "\n",
    "    Parameters:\n",
    "    texts (list or pd.Series): The dataset or list of text entries to display.\n",
    "    n (int): The number of rows to display. Default is 5.\n",
    "\n",
    "    Returns:\n",
    "    None\n",
    "    \"\"\"\n",
    "    if isinstance(texts, (list, pd.Series)):\n",
    "        data = pd.DataFrame({\"Text\": texts[:n]})\n",
    "        data[\"Word Count\"] = data[\"Text\"].apply(lambda x: len(x.split()))\n",
    "        data[\"Character Count\"] = data[\"Text\"].apply(len)\n",
    "        display(data)\n",
    "\n",
    "        # Plotting word counts for quick overview\n",
    "        plt.figure(figsize=(8, 5))\n",
    "        plt.bar(range(n), data[\"Word Count\"], color=\"skyblue\")\n",
    "        plt.xticks(range(n), [f\"Text {i+1}\" for i in range(n)], rotation=45)\n",
    "        plt.xlabel(\"Text Entries\")\n",
    "        plt.ylabel(\"Word Count\")\n",
    "        plt.title(\"Word Count of First Few Text Entries\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        raise ValueError(\"The input should be a list or pandas Series of text entries.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Gaurav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Gaurav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Gaurav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# text_preprocessing_toolkit/preprocessing.py\n",
    "\n",
    "\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")  # Optional, improves WordNet lemmatization\n",
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.stopwords = set(stopwords.words(\"english\"))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.spell = SpellChecker()\n",
    "        self.ps = PorterStemmer()\n",
    "        self.wordnet_map = {\n",
    "            \"N\": wordnet.NOUN,\n",
    "            \"V\": wordnet.VERB,\n",
    "            \"J\": wordnet.ADJ,\n",
    "            \"R\": wordnet.ADV,\n",
    "        }\n",
    "\n",
    "    def remove_punctuation(self, text):\n",
    "        return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        return \" \".join(\n",
    "            [word for word in str(text).split() if word not in self.stopwords]\n",
    "        )\n",
    "\n",
    "    def get_frequent_words(self, texts, top_n=10):\n",
    "        word_counts = Counter()\n",
    "        for text in texts:\n",
    "            for word in text.split():\n",
    "                word_counts[word] += 1\n",
    "        return set([word for word, count in word_counts.most_common(top_n)])\n",
    "\n",
    "    def remove_frequent_words(self, text, frequent_words):\n",
    "        return \" \".join(\n",
    "            [word for word in str(text).split() if word not in frequent_words]\n",
    "        )\n",
    "\n",
    "    def get_rare_words(self, texts, bottom_n=10):\n",
    "        word_counts = Counter()\n",
    "        for text in texts:\n",
    "            for word in text.split():\n",
    "                word_counts[word] += 1\n",
    "        return set(\n",
    "            word for word, count in word_counts.most_common()[: -bottom_n - 1 : -1]\n",
    "        )\n",
    "\n",
    "    def remove_rare_words(self, text, rare_words):\n",
    "        return \" \".join([word for word in str(text).split() if word not in rare_words])\n",
    "\n",
    "    def remove_special_characters(self, text):\n",
    "        text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        return text\n",
    "\n",
    "    def stem_text(self, text):\n",
    "        return \" \".join([self.ps.stem(word) for word in text.split()])\n",
    "\n",
    "    def lemmatize_text(self, text):\n",
    "        pos_text = pos_tag(text.split())\n",
    "        return \" \".join(\n",
    "            [\n",
    "                self.lemmatizer.lemmatize(\n",
    "                    word, self.wordnet_map.get(pos[0], wordnet.NOUN)\n",
    "                )\n",
    "                for word, pos in pos_text\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # def lemmatize_text(self, text):\n",
    "    #     pos_text = pos_tag(text.split())\n",
    "    #     return \" \".join([self.lemmatizer.lemmatize(word, self.wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_text])\n",
    "\n",
    "    def remove_url(self, text):\n",
    "        return re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
    "\n",
    "    def remove_html_tags(self, text):\n",
    "        return re.sub(r\"<[^>]+>\", \"\", text)\n",
    "\n",
    "    def correct_spellings(self, text):\n",
    "        corrected_text = []\n",
    "        misspelled_words = self.spell.unknown(text.split())\n",
    "        for word in text.split():\n",
    "            if word in misspelled_words:\n",
    "                corrected_word = self.spell.correction(word)\n",
    "                corrected_text.append(corrected_word)\n",
    "            else:\n",
    "                corrected_text.append(word)\n",
    "        return \" \".join(corrected_text)\n",
    "\n",
    "    def lowercase(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    def preprocess(self, text, steps=None):\n",
    "        if steps is None:\n",
    "            steps = [\n",
    "                \"lowercase\",\n",
    "                \"remove_punctuation\",\n",
    "                \"remove_stopwords\",\n",
    "                \"remove_special_characters\",\n",
    "                \"remove_url\",\n",
    "                \"remove_html_tags\",\n",
    "                \"correct_spellings\",\n",
    "                \"lemmatize_text\",\n",
    "            ]\n",
    "        for step in steps:\n",
    "            text = getattr(self, step)(text)\n",
    "        return text\n",
    "\n",
    "    def head(self, texts, n=5):\n",
    "        \"\"\"\n",
    "        Display a summary of the first few entries of the dataset for quick visualization.\n",
    "\n",
    "        Parameters:\n",
    "        texts (list or pd.Series): The dataset or list of text entries to display.\n",
    "        n (int): The number of rows to display. Default is 5.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        if isinstance(texts, (list, pd.Series)):\n",
    "            data = pd.DataFrame({\"Text\": texts[:n]})\n",
    "            data[\"Word Count\"] = data[\"Text\"].apply(lambda x: len(x.split()))\n",
    "            data[\"Character Count\"] = data[\"Text\"].apply(len)\n",
    "            display(data)\n",
    "\n",
    "            # Plotting word counts for quick overview\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.bar(range(n), data[\"Word Count\"], color=\"skyblue\")\n",
    "            plt.xticks(range(n), [f\"Text {i+1}\" for i in range(n)], rotation=45)\n",
    "            plt.xlabel(\"Text Entries\")\n",
    "            plt.ylabel(\"Word Count\")\n",
    "            plt.title(\"Word Count of First Few Text Entries\")\n",
    "            plt.show()\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"The input should be a list or pandas Series of text entries.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing punctuation: Hello world This is a test text with HTML html tags and a URL httpsexamplecom\n",
      "Lowercase text: hello, world! this is a test text with html <html> tags and a url: https://example.com.\n",
      "After removing stopwords: Hello, world! This test text HTML <html> tags URL: https://example.com.\n",
      "After removing URLs: Hello, world! This is a test text with HTML <html> tags and a URL: \n",
      "After removing HTML tags: Hello, world! This is a test text with HTML  tags and a URL: https://example.com.\n",
      "After spell correction: This is a test text for spelling correction\n",
      "After stemming: run jump play better\n"
     ]
    }
   ],
   "source": [
    "processor = TextPreprocessor()\n",
    "\n",
    "# Sample text\n",
    "text = \"Hello, world! This is a test text with HTML <html> tags and a URL: https://example.com.\"\n",
    "\n",
    "# Removing punctuation\n",
    "no_punctuation_text = processor.remove_punctuation(text)\n",
    "print(\"After removing punctuation:\", no_punctuation_text)\n",
    "\n",
    "# Lowercasing text\n",
    "lowercase_text = processor.lowercase(text)\n",
    "print(\"Lowercase text:\", lowercase_text)\n",
    "\n",
    "# Removing stopwords\n",
    "no_stopwords_text = processor.remove_stopwords(text)\n",
    "print(\"After removing stopwords:\", no_stopwords_text)\n",
    "\n",
    "# Removing URLs\n",
    "no_urls_text = processor.remove_url(text)\n",
    "print(\"After removing URLs:\", no_urls_text)\n",
    "\n",
    "# Removing HTML tags\n",
    "no_html_text = processor.remove_html_tags(text)\n",
    "print(\"After removing HTML tags:\", no_html_text)\n",
    "\n",
    "# Correcting spellings\n",
    "corrected_text = processor.correct_spellings(\"This is a tst txt for spellng corection\")\n",
    "print(\"After spell correction:\", corrected_text)\n",
    "\n",
    "# Lemmatizing text\n",
    "# lemmatized_text = processor.lemmatize_text(\"running jumped plays better\")\n",
    "# print(\"After lemmatization:\", lemmatized_text)\n",
    "\n",
    "\n",
    "# Stemming text\n",
    "stemmed_text = processor.stem_text(\"running jumped plays better\")\n",
    "print(\"After stemming:\", stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'DataFrame'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[90], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m t \u001b[38;5;241m=\u001b[39m \u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataFrame\u001b[49m(text)\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'str' object has no attribute 'DataFrame'"
     ]
    }
   ],
   "source": [
    "t = text.DataFrame(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The input should be a list or pandas Series of text entries.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[88], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhead\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[79], line 126\u001b[0m, in \u001b[0;36mTextPreprocessor.head\u001b[1;34m(self, texts, n)\u001b[0m\n\u001b[0;32m    124\u001b[0m     plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[0;32m    125\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 126\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe input should be a list or pandas Series of text entries.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: The input should be a list or pandas Series of text entries."
     ]
    }
   ],
   "source": [
    "processor.head(text, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
