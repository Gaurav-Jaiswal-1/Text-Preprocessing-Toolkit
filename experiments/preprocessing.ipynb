{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = (\n",
    "    \"Text preprocessing is a crucial step in natural language processing (NLP) and machine learning workflows, enabling more accurate and efficient text analysis. Raw text data is often messy and unstructured, containing inconsistencies, irrelevant characters, stop words, and varied forms of words. Through text preprocessing, this data can be cleaned, standardized, and transformed into a format better suited for algorithms to understand and analyze. Key steps include tokenization, which breaks down text into individual words or tokens; lemmatization, which converts words to their base or root forms; and removing stop words, which are common words (like \"\n",
    "    'the,\" \"and,\" \"is'\n",
    "    \") that don’t add substantial meaning to the text. Additionally, text normalization techniques—such as lowercasing and punctuation removal—help in reducing dimensionality and ensuring consistency. Effective text preprocessing not only improves model performance by reducing noise and redundancy but also enhances the interpretability of text-based insights in applications like sentiment analysis, document classification, and chatbot responses.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Text preprocessing is a crucial step in natura...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   0\n",
       "0  Text preprocessing is a crucial step in natura..."
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import string\n",
    "\n",
    "df = pd.DataFrame([text])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"text\"] = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[0], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Text preprocessing is a crucial step in natura...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text\n",
       "0  Text preprocessing is a crucial step in natura..."
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Convert to lowercase**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"cleaned_text\"] = df[\"text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Text preprocessing is a crucial step in natura...</td>\n",
       "      <td>text preprocessing is a crucial step in natura...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Text preprocessing is a crucial step in natura...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  text preprocessing is a crucial step in natura...  "
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Remove Punctuation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'!\"#$%&\\'()*+,-./:;<=>?@[\\\\]^_`{|}~'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "string.punctuation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    punctuations = string.punctuation\n",
    "    return text.translate(str.maketrans(\"\", \"\", punctuations))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Text preprocessing is a crucial step in natura...</td>\n",
       "      <td>text preprocessing is a crucial step in natura...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Text preprocessing is a crucial step in natura...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  text preprocessing is a crucial step in natura...  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"cleaned_text\"] = df[\"cleaned_text\"].apply(lambda x: remove_punctuation(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Remove stop words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his',\n",
       " 'himself',\n",
       " 'she',\n",
       " \"she's\",\n",
       " 'her',\n",
       " 'hers',\n",
       " 'herself',\n",
       " 'it',\n",
       " \"it's\",\n",
       " 'its',\n",
       " 'itself',\n",
       " 'they',\n",
       " 'them',\n",
       " 'their',\n",
       " 'theirs',\n",
       " 'themselves',\n",
       " 'what',\n",
       " 'which',\n",
       " 'who',\n",
       " 'whom',\n",
       " 'this',\n",
       " 'that',\n",
       " \"that'll\",\n",
       " 'these',\n",
       " 'those',\n",
       " 'am',\n",
       " 'is',\n",
       " 'are',\n",
       " 'was',\n",
       " 'were',\n",
       " 'be',\n",
       " 'been',\n",
       " 'being',\n",
       " 'have',\n",
       " 'has',\n",
       " 'had',\n",
       " 'having',\n",
       " 'do',\n",
       " 'does',\n",
       " 'did',\n",
       " 'doing',\n",
       " 'a',\n",
       " 'an',\n",
       " 'the',\n",
       " 'and',\n",
       " 'but',\n",
       " 'if',\n",
       " 'or',\n",
       " 'because',\n",
       " 'as',\n",
       " 'until',\n",
       " 'while',\n",
       " 'of',\n",
       " 'at',\n",
       " 'by',\n",
       " 'for',\n",
       " 'with',\n",
       " 'about',\n",
       " 'against',\n",
       " 'between',\n",
       " 'into',\n",
       " 'through',\n",
       " 'during',\n",
       " 'before',\n",
       " 'after',\n",
       " 'above',\n",
       " 'below',\n",
       " 'to',\n",
       " 'from',\n",
       " 'up',\n",
       " 'down',\n",
       " 'in',\n",
       " 'out',\n",
       " 'on',\n",
       " 'off',\n",
       " 'over',\n",
       " 'under',\n",
       " 'again',\n",
       " 'further',\n",
       " 'then',\n",
       " 'once',\n",
       " 'here',\n",
       " 'there',\n",
       " 'when',\n",
       " 'where',\n",
       " 'why',\n",
       " 'how',\n",
       " 'all',\n",
       " 'any',\n",
       " 'both',\n",
       " 'each',\n",
       " 'few',\n",
       " 'more',\n",
       " 'most',\n",
       " 'other',\n",
       " 'some',\n",
       " 'such',\n",
       " 'no',\n",
       " 'nor',\n",
       " 'not',\n",
       " 'only',\n",
       " 'own',\n",
       " 'same',\n",
       " 'so',\n",
       " 'than',\n",
       " 'too',\n",
       " 'very',\n",
       " 's',\n",
       " 't',\n",
       " 'can',\n",
       " 'will',\n",
       " 'just',\n",
       " 'don',\n",
       " \"don't\",\n",
       " 'should',\n",
       " \"should've\",\n",
       " 'now',\n",
       " 'd',\n",
       " 'll',\n",
       " 'm',\n",
       " 'o',\n",
       " 're',\n",
       " 've',\n",
       " 'y',\n",
       " 'ain',\n",
       " 'aren',\n",
       " \"aren't\",\n",
       " 'couldn',\n",
       " \"couldn't\",\n",
       " 'didn',\n",
       " \"didn't\",\n",
       " 'doesn',\n",
       " \"doesn't\",\n",
       " 'hadn',\n",
       " \"hadn't\",\n",
       " 'hasn',\n",
       " \"hasn't\",\n",
       " 'haven',\n",
       " \"haven't\",\n",
       " 'isn',\n",
       " \"isn't\",\n",
       " 'ma',\n",
       " 'mightn',\n",
       " \"mightn't\",\n",
       " 'mustn',\n",
       " \"mustn't\",\n",
       " 'needn',\n",
       " \"needn't\",\n",
       " 'shan',\n",
       " \"shan't\",\n",
       " 'shouldn',\n",
       " \"shouldn't\",\n",
       " 'wasn',\n",
       " \"wasn't\",\n",
       " 'weren',\n",
       " \"weren't\",\n",
       " 'won',\n",
       " \"won't\",\n",
       " 'wouldn',\n",
       " \"wouldn't\"]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "\n",
    "stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "STOPWORDS = set(stopwords.words(\"english\"))\n",
    "\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in STOPWORDS])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Text preprocessing is a crucial step in natura...</td>\n",
       "      <td>text preprocessing crucial step natural langua...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Text preprocessing is a crucial step in natura...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  text preprocessing crucial step natural langua...  "
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[\"cleaned_text\"] = df[\"cleaned_text\"].apply(lambda x: remove_stopwords(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Remove Frequent words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('text', 8),\n",
       " ('words', 6),\n",
       " ('preprocessing', 3),\n",
       " ('analysis', 2),\n",
       " ('data', 2),\n",
       " ('stop', 2),\n",
       " ('forms', 2),\n",
       " ('like', 2),\n",
       " ('reducing', 2),\n",
       " ('crucial', 1)]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "word_counts = Counter()\n",
    "for text in df[\"cleaned_text\"]:\n",
    "    for word in text.split():\n",
    "        word_counts[word] += 1\n",
    "word_counts.most_common(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Text preprocessing is a crucial step in natura...</td>\n",
       "      <td>step natural language processing nlp machine l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Text preprocessing is a crucial step in natura...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  step natural language processing nlp machine l...  "
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "FREQUENT_WORDS = set([word for (word, count) in word_counts.most_common(10)])\n",
    "\n",
    "\n",
    "def remove_frequent_words(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in FREQUENT_WORDS])\n",
    "\n",
    "\n",
    "df[\"cleaned_text\"] = df[\"cleaned_text\"].apply(lambda x: remove_frequent_words(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Remove Rare Words**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'applications',\n",
       " 'chatbot',\n",
       " 'classification',\n",
       " 'document',\n",
       " 'insights',\n",
       " 'interpretability',\n",
       " 'responses',\n",
       " 'sentiment',\n",
       " 'textbased'}"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "RARE_WORDS = set(word for (word, count) in word_counts.most_common()[:-10:-1])\n",
    "RARE_WORDS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Text preprocessing is a crucial step in natura...</td>\n",
       "      <td>step natural language processing nlp machine l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Text preprocessing is a crucial step in natura...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  step natural language processing nlp machine l...  "
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_rare_words(text):\n",
    "    return \" \".join([word for word in str(text).split() if word not in RARE_WORDS])\n",
    "\n",
    "\n",
    "df[\"cleaned_text\"] = df[\"cleaned_text\"].apply(lambda x: remove_rare_words(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Remove Special Character**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Text preprocessing is a crucial step in natura...</td>\n",
       "      <td>step natural language processing nlp machine l...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Text preprocessing is a crucial step in natura...   \n",
       "\n",
       "                                        cleaned_text  \n",
       "0  step natural language processing nlp machine l...  "
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def remove_special_characters(text):\n",
    "    text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "    text = re.sub(r\"\\s+\", \" \", text)\n",
    "    return text\n",
    "\n",
    "\n",
    "df[\"cleaned_text\"] = df[\"cleaned_text\"].apply(lambda x: remove_special_characters(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>stemmed_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Text preprocessing is a crucial step in natura...</td>\n",
       "      <td>step natur languag process nlp machin learn wo...</td>\n",
       "      <td>step natur languag process nlp machin learn wo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  \\\n",
       "0  Text preprocessing is a crucial step in natura...   \n",
       "\n",
       "                                        cleaned_text  \\\n",
       "0  step natur languag process nlp machin learn wo...   \n",
       "\n",
       "                                        stemmed_text  \n",
       "0  step natur languag process nlp machin learn wo...  "
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "ps = PorterStemmer()\n",
    "\n",
    "\n",
    "def stem_text(text):\n",
    "    return \" \".join([ps.stem(word) for word in text.split()])\n",
    "\n",
    "\n",
    "df[\"stemmed_text\"] = df[\"cleaned_text\"].apply(lambda x: stem_text(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Lemmatization and POS tagging**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Error loading averaged_perceptron_tagger_english: Package\n",
      "[nltk_data]     'averaged_perceptron_tagger_english' not found in\n",
      "[nltk_data]     index\n"
     ]
    }
   ],
   "source": [
    "from nltk import pos_tag\n",
    "from nltk.corpus import wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "wordnet_map = {\"N\": wordnet.NOUN, \"V\": wordnet.VERB, \"J\": wordnet.ADJ, \"R\": wordnet.ADV}\n",
    "\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    pos_text = pos_tag(text.split())\n",
    "    return \" \".join(\n",
    "        [\n",
    "            lemmatizer.lemmatize(word, wordnet_map.get(pos[0], wordnet.NOUN))\n",
    "            for word, pos in pos_text\n",
    "        ]\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "ename": "LookupError",
     "evalue": "\n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Gaurav/nltk_data'\n    - 'c:\\\\Users\\\\Gaurav\\\\OneDrive\\\\Desktop\\\\TPT\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\Gaurav\\\\OneDrive\\\\Desktop\\\\TPT\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Gaurav\\\\OneDrive\\\\Desktop\\\\TPT\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Gaurav\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[68], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemmatized_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcleaned_text\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mlemmatize_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "File \u001b[1;32mc:\\Users\\Gaurav\\OneDrive\\Desktop\\TPT\\.venv\\lib\\site-packages\\pandas\\core\\series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[0;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4529\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Gaurav\\OneDrive\\Desktop\\TPT\\.venv\\lib\\site-packages\\pandas\\core\\apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[0;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[1;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Gaurav\\OneDrive\\Desktop\\TPT\\.venv\\lib\\site-packages\\pandas\\core\\apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32mc:\\Users\\Gaurav\\OneDrive\\Desktop\\TPT\\.venv\\lib\\site-packages\\pandas\\_libs\\lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[68], line 1\u001b[0m, in \u001b[0;36m<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlemmatized_text\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcleaned_text\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mlemmatize_text\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      2\u001b[0m df\u001b[38;5;241m.\u001b[39mhead()\n",
      "Cell \u001b[1;32mIn[67], line 12\u001b[0m, in \u001b[0;36mlemmatize_text\u001b[1;34m(text)\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlemmatize_text\u001b[39m(text):\n\u001b[1;32m---> 12\u001b[0m   pos_text \u001b[38;5;241m=\u001b[39m \u001b[43mpos_tag\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     13\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mjoin([lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(word, wordnet_map\u001b[38;5;241m.\u001b[39mget(pos[\u001b[38;5;241m0\u001b[39m], wordnet\u001b[38;5;241m.\u001b[39mNOUN)) \u001b[38;5;28;01mfor\u001b[39;00m word, pos \u001b[38;5;129;01min\u001b[39;00m pos_text])\n",
      "File \u001b[1;32mc:\\Users\\Gaurav\\OneDrive\\Desktop\\TPT\\.venv\\lib\\site-packages\\nltk\\tag\\__init__.py:168\u001b[0m, in \u001b[0;36mpos_tag\u001b[1;34m(tokens, tagset, lang)\u001b[0m\n\u001b[0;32m    143\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpos_tag\u001b[39m(tokens, tagset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    144\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;124;03m    Use NLTK's currently recommended part of speech tagger to\u001b[39;00m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;124;03m    tag the given list of tokens.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    166\u001b[0m \u001b[38;5;124;03m    :rtype: list(tuple(str, str))\u001b[39;00m\n\u001b[0;32m    167\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 168\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43m_get_tagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    169\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _pos_tag(tokens, tagset, tagger, lang)\n",
      "File \u001b[1;32mc:\\Users\\Gaurav\\OneDrive\\Desktop\\TPT\\.venv\\lib\\site-packages\\nltk\\tag\\__init__.py:110\u001b[0m, in \u001b[0;36m_get_tagger\u001b[1;34m(lang)\u001b[0m\n\u001b[0;32m    108\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m PerceptronTagger(lang\u001b[38;5;241m=\u001b[39mlang)\n\u001b[0;32m    109\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 110\u001b[0m     tagger \u001b[38;5;241m=\u001b[39m \u001b[43mPerceptronTagger\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tagger\n",
      "File \u001b[1;32mc:\\Users\\Gaurav\\OneDrive\\Desktop\\TPT\\.venv\\lib\\site-packages\\nltk\\tag\\perceptron.py:183\u001b[0m, in \u001b[0;36mPerceptronTagger.__init__\u001b[1;34m(self, load, lang)\u001b[0m\n\u001b[0;32m    181\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclasses \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n\u001b[0;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m load:\n\u001b[1;32m--> 183\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mload_from_json\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlang\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Gaurav\\OneDrive\\Desktop\\TPT\\.venv\\lib\\site-packages\\nltk\\tag\\perceptron.py:273\u001b[0m, in \u001b[0;36mPerceptronTagger.load_from_json\u001b[1;34m(self, lang)\u001b[0m\n\u001b[0;32m    271\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mload_from_json\u001b[39m(\u001b[38;5;28mself\u001b[39m, lang\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124meng\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    272\u001b[0m     \u001b[38;5;66;03m# Automatically find path to the tagger if location is not specified.\u001b[39;00m\n\u001b[1;32m--> 273\u001b[0m     loc \u001b[38;5;241m=\u001b[39m \u001b[43mfind\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtaggers/averaged_perceptron_tagger_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mlang\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    274\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(loc \u001b[38;5;241m+\u001b[39m TAGGER_JSONS[lang][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mweights\u001b[39m\u001b[38;5;124m\"\u001b[39m]) \u001b[38;5;28;01mas\u001b[39;00m fin:\n\u001b[0;32m    275\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m=\u001b[39m json\u001b[38;5;241m.\u001b[39mload(fin)\n",
      "File \u001b[1;32mc:\\Users\\Gaurav\\OneDrive\\Desktop\\TPT\\.venv\\lib\\site-packages\\nltk\\data.py:579\u001b[0m, in \u001b[0;36mfind\u001b[1;34m(resource_name, paths)\u001b[0m\n\u001b[0;32m    577\u001b[0m sep \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m*\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m70\u001b[39m\n\u001b[0;32m    578\u001b[0m resource_not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmsg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00msep\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 579\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mLookupError\u001b[39;00m(resource_not_found)\n",
      "\u001b[1;31mLookupError\u001b[0m: \n**********************************************************************\n  Resource \u001b[93maveraged_perceptron_tagger_eng\u001b[0m not found.\n  Please use the NLTK Downloader to obtain the resource:\n\n  \u001b[31m>>> import nltk\n  >>> nltk.download('averaged_perceptron_tagger_eng')\n  \u001b[0m\n  For more information see: https://www.nltk.org/data.html\n\n  Attempted to load \u001b[93mtaggers/averaged_perceptron_tagger_eng/\u001b[0m\n\n  Searched in:\n    - 'C:\\\\Users\\\\Gaurav/nltk_data'\n    - 'c:\\\\Users\\\\Gaurav\\\\OneDrive\\\\Desktop\\\\TPT\\\\.venv\\\\nltk_data'\n    - 'c:\\\\Users\\\\Gaurav\\\\OneDrive\\\\Desktop\\\\TPT\\\\.venv\\\\share\\\\nltk_data'\n    - 'c:\\\\Users\\\\Gaurav\\\\OneDrive\\\\Desktop\\\\TPT\\\\.venv\\\\lib\\\\nltk_data'\n    - 'C:\\\\Users\\\\Gaurav\\\\AppData\\\\Roaming\\\\nltk_data'\n    - 'C:\\\\nltk_data'\n    - 'D:\\\\nltk_data'\n    - 'E:\\\\nltk_data'\n**********************************************************************\n"
     ]
    }
   ],
   "source": [
    "df[\"lemmatized_text\"] = df[\"cleaned_text\"].apply(lambda x: lemmatize_text(x))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Remove URL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"https://www.google.com is a website\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' is a website'"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_url(text):\n",
    "    return re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
    "\n",
    "\n",
    "remove_url(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Remove HTML Tags**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"<html><body><h1>My First Heading </h1><p>My first paragraph.</p></body></html>\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'My First Heading My first paragraph.'"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def remove_html_tags(text):\n",
    "    return re.sub(r\"<[^>]+>\", \"\", text)\n",
    "\n",
    "\n",
    "remove_html_tags(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Spelling Correction**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \" welcme to my wordl. mye naame is Gaurav\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spellchecker import SpellChecker\n",
    "\n",
    "spell = SpellChecker()\n",
    "\n",
    "\n",
    "def correct_spellings(text):\n",
    "    corrected_text = []\n",
    "    misspelled_words = spell.unknown(text.split())\n",
    "    for word in text.split():\n",
    "        if word in misspelled_words:\n",
    "            corrected_word = spell.correction(word)\n",
    "            corrected_text.append(corrected_word)\n",
    "        else:\n",
    "            corrected_text.append(word)\n",
    "    return \" \".join(corrected_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'welcome to my world my name is Gaurav'"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "correct_spellings(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     C:\\Users\\Gaurav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Gaurav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Gaurav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# text_preprocessing_toolkit/preprocessing.py\n",
    "\n",
    "\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from spellchecker import SpellChecker\n",
    "\n",
    "import nltk\n",
    "\n",
    "nltk.download(\"averaged_perceptron_tagger_eng\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")  # Optional, improves WordNet lemmatization\n",
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(\n",
    "        self,\n",
    "        remove_punctuation=True,\n",
    "        remove_stopwords=True,\n",
    "        get_frequent_words=True,\n",
    "        remove_frequent_words=True,\n",
    "        get_rare_words=True,\n",
    "        remove_rare_words=True,\n",
    "        remove_special_characters=True,\n",
    "        stem_text=True,\n",
    "        lemmatize_text=True,\n",
    "        correct_spellings=True,\n",
    "    ):\n",
    "        self.stopwords = set(stopwords.words(\"english\"))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.spell = SpellChecker()\n",
    "        self.ps = PorterStemmer()\n",
    "        self.wordnet_map = {\n",
    "            \"N\": wordnet.NOUN,\n",
    "            \"V\": wordnet.VERB,\n",
    "            \"J\": wordnet.ADJ,\n",
    "            \"R\": wordnet.ADV,\n",
    "        }\n",
    "\n",
    "    def remove_punctuation(self, text):\n",
    "        return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        return \" \".join(\n",
    "            [word for word in str(text).split() if word not in self.stopwords]\n",
    "        )\n",
    "\n",
    "    def get_frequent_words(self, texts, top_n=10):\n",
    "        word_counts = Counter()\n",
    "        for text in texts:\n",
    "            for word in text.split():\n",
    "                word_counts[word] += 1\n",
    "        return set([word for word, count in word_counts.most_common(top_n)])\n",
    "\n",
    "    def remove_frequent_words(self, text, frequent_words):\n",
    "        return \" \".join(\n",
    "            [word for word in str(text).split() if word not in frequent_words]\n",
    "        )\n",
    "\n",
    "    def get_rare_words(self, texts, bottom_n=10):\n",
    "        word_counts = Counter()\n",
    "        for text in texts:\n",
    "            for word in text.split():\n",
    "                word_counts[word] += 1\n",
    "        return set(\n",
    "            word for word, count in word_counts.most_common()[: -bottom_n - 1 : -1]\n",
    "        )\n",
    "\n",
    "    def remove_rare_words(self, text, rare_words):\n",
    "        return \" \".join([word for word in str(text).split() if word not in rare_words])\n",
    "\n",
    "    def remove_special_characters(self, text):\n",
    "        text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        return text\n",
    "\n",
    "    def stem_text(self, text):\n",
    "        return \" \".join([self.ps.stem(word) for word in text.split()])\n",
    "\n",
    "    def lemmatize_text(self, text):\n",
    "        pos_text = pos_tag(text.split())\n",
    "        return \" \".join(\n",
    "            [\n",
    "                self.lemmatizer.lemmatize(\n",
    "                    word, self.wordnet_map.get(pos[0], wordnet.NOUN)\n",
    "                )\n",
    "                for word, pos in pos_text\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # def lemmatize_text(self, text):\n",
    "    #     pos_text = pos_tag(text.split())\n",
    "    #     return \" \".join([self.lemmatizer.lemmatize(word, self.wordnet_map.get(pos[0], wordnet.NOUN)) for word, pos in pos_text])\n",
    "\n",
    "    def remove_url(self, text):\n",
    "        return re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
    "\n",
    "    def remove_html_tags(self, text):\n",
    "        return re.sub(r\"<[^>]+>\", \"\", text)\n",
    "\n",
    "    def correct_spellings(self, text):\n",
    "        corrected_text = []\n",
    "        misspelled_words = self.spell.unknown(text.split())\n",
    "        for word in text.split():\n",
    "            if word in misspelled_words:\n",
    "                corrected_word = self.spell.correction(word)\n",
    "                corrected_text.append(corrected_word)\n",
    "            else:\n",
    "                corrected_text.append(word)\n",
    "        return \" \".join(corrected_text)\n",
    "\n",
    "    def lowercase(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    def preprocess(self, text, steps=None):\n",
    "        if steps is None:\n",
    "            steps = [\n",
    "                \"lowercase\",\n",
    "                \"remove_punctuation\",\n",
    "                \"remove_stopwords\",\n",
    "                \"remove_special_characters\",\n",
    "                \"remove_url\",\n",
    "                \"remove_html_tags\",\n",
    "                \"correct_spellings\",\n",
    "                \"lemmatize_text\",\n",
    "            ]\n",
    "        for step in steps:\n",
    "            text = getattr(self, step)(text)\n",
    "        return text\n",
    "\n",
    "    def head(self, texts, n=5):\n",
    "        \"\"\"\n",
    "        Display the first few entries of the dataset for quick visualization.\n",
    "\n",
    "        Parameters:\n",
    "        texts (list or pd.Series): The dataset or list of text entries to display.\n",
    "        n (int): The number of rows to display. Default is 5.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        if isinstance(texts, (list, pd.Series)):\n",
    "            data = pd.DataFrame({\"Text\": texts[:n]})\n",
    "            print(data)\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"The input should be a list or pandas Series of text entries.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = [\n",
    "    \"Text preprocessing is a crucial step in natural language processing (NLP) and machine learning workflows, enabling more accurate and efficient text analysis. Raw text data is often messy and unstructured, containing inconsistencies, irrelevant characters, stop words, and varied forms of words.\",\n",
    "    \"Through text preprocessing, this data can be cleaned, standardized, and transformed into a format better suited for algorithms to understand and analyze.\",\n",
    "    \" Key steps include tokenization, which breaks down text into individual words or tokens; lemmatization, which converts words to their base or root forms; and removing stop words, which are common words (like \"\n",
    "    'the,\" \"and,\" \"is'\n",
    "    \") that don’t add substantial meaning to the text.\",\n",
    "    \"Additionally, text normalization techniques—such as lowercasing and punctuation removal—help in reducing dimensionality and ensuring consistency.\",\n",
    "    \"Effective text preprocessing not only improves model performance by reducing noise and redundancy but also enhances the interpretability of text-based insights in applications like sentiment analysis, document classification, and chatbot responses.\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing punctuation: Hello world This is a test text with HTML html tags and a URL httpsexamplecom\n",
      "Lowercase text: hello, world! this is a test text with html <html> tags and a url: https://example.com.\n",
      "After removing stopwords: Hello, world! This test text HTML <html> tags URL: https://example.com.\n",
      "After removing URLs: Hello, world! This is a test text with HTML <html> tags and a URL: \n",
      "After removing HTML tags: Hello, world! This is a test text with HTML  tags and a URL: https://example.com.\n",
      "After spell correction: This is a test text for spelling correction\n",
      "After lemmatization: run jump play well\n",
      "After stemming: run jump play better\n"
     ]
    }
   ],
   "source": [
    "processor = TextPreprocessor()\n",
    "\n",
    "# Sample text\n",
    "text = \"Hello, world! This is a test text with HTML <html> tags and a URL: https://example.com.\"\n",
    "\n",
    "# Removing punctuation\n",
    "no_punctuation_text = processor.remove_punctuation(text)\n",
    "print(\"After removing punctuation:\", no_punctuation_text)\n",
    "\n",
    "# Lowercasing text\n",
    "lowercase_text = processor.lowercase(text)\n",
    "print(\"Lowercase text:\", lowercase_text)\n",
    "\n",
    "# Removing stopwords\n",
    "no_stopwords_text = processor.remove_stopwords(text)\n",
    "print(\"After removing stopwords:\", no_stopwords_text)\n",
    "\n",
    "# Removing URLs\n",
    "no_urls_text = processor.remove_url(text)\n",
    "print(\"After removing URLs:\", no_urls_text)\n",
    "\n",
    "# Removing HTML tags\n",
    "no_html_text = processor.remove_html_tags(text)\n",
    "print(\"After removing HTML tags:\", no_html_text)\n",
    "\n",
    "# Correcting spellings\n",
    "corrected_text = processor.correct_spellings(\"This is a tst txt for spellng corection\")\n",
    "print(\"After spell correction:\", corrected_text)\n",
    "\n",
    "# Lemmatizing text\n",
    "lemmatized_text = processor.lemmatize_text(\"running jumped plays better\")\n",
    "print(\"After lemmatization:\", lemmatized_text)\n",
    "\n",
    "\n",
    "# Stemming text\n",
    "stemmed_text = processor.stem_text(\"running jumped plays better\")\n",
    "print(\"After stemming:\", stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 8: expected str instance, NoneType found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[120], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[116], line 98\u001b[0m, in \u001b[0;36mTextPreprocessor.preprocess\u001b[1;34m(self, text, steps)\u001b[0m\n\u001b[0;32m     92\u001b[0m     steps \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     93\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlowercase\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mremove_punctuation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mremove_stopwords\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     94\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mremove_special_characters\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mremove_url\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mremove_html_tags\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     95\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcorrect_spellings\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlemmatize_text\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     96\u001b[0m     ]\n\u001b[0;32m     97\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m steps:\n\u001b[1;32m---> 98\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     99\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "Cell \u001b[1;32mIn[116], line 85\u001b[0m, in \u001b[0;36mTextPreprocessor.correct_spellings\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m         corrected_text\u001b[38;5;241m.\u001b[39mappend(word)\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorrected_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 8: expected str instance, NoneType found"
     ]
    }
   ],
   "source": [
    "processor.preprocess(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Gaurav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Gaurav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Gaurav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gaurav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# text_preprocessing_toolkit/preprocessing.py\n",
    "\n",
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from spellchecker import SpellChecker\n",
    "import nltk\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Download required NLTK resources if not already downloaded\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self):\n",
    "        self.stopwords = set(stopwords.words(\"english\"))\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.spell = SpellChecker()\n",
    "        self.ps = PorterStemmer()\n",
    "        self.wordnet_map = {\n",
    "            \"N\": wordnet.NOUN,\n",
    "            \"V\": wordnet.VERB,\n",
    "            \"J\": wordnet.ADJ,\n",
    "            \"R\": wordnet.ADV,\n",
    "        }\n",
    "\n",
    "    def remove_punctuation(self, text):\n",
    "        return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    def remove_stopwords(self, text):\n",
    "        return \" \".join([word for word in text.split() if word not in self.stopwords])\n",
    "\n",
    "    def remove_special_characters(self, text):\n",
    "        text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "        text = re.sub(r\"\\s+\", \" \", text)\n",
    "        return text\n",
    "\n",
    "    def stem_text(self, text):\n",
    "        return \" \".join([self.ps.stem(word) for word in text.split()])\n",
    "\n",
    "    def lemmatize_text(self, text):\n",
    "        pos_text = pos_tag(text.split())\n",
    "        return \" \".join(\n",
    "            [\n",
    "                self.lemmatizer.lemmatize(\n",
    "                    word, self.wordnet_map.get(pos[0], wordnet.NOUN)\n",
    "                )\n",
    "                for word, pos in pos_text\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def remove_url(self, text):\n",
    "        return re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
    "\n",
    "    def remove_html_tags(self, text):\n",
    "        return re.sub(r\"<[^>]+>\", \"\", text)\n",
    "\n",
    "    def correct_spellings(self, text):\n",
    "        corrected_text = []\n",
    "        misspelled_words = self.spell.unknown(text.split())\n",
    "        for word in text.split():\n",
    "            if word in misspelled_words:\n",
    "                corrected_word = self.spell.correction(word)\n",
    "                corrected_text.append(corrected_word)\n",
    "            else:\n",
    "                corrected_text.append(word)\n",
    "        return \" \".join(corrected_text)\n",
    "\n",
    "    def lowercase(self, text):\n",
    "        return text.lower()\n",
    "\n",
    "    def preprocess(self, text, custom_steps=None):\n",
    "        \"\"\"\n",
    "        Automatically preprocess text with a default pipeline.\n",
    "        User can specify custom_steps for specific preprocessing order.\n",
    "\n",
    "        Parameters:\n",
    "        text (str): Text to preprocess.\n",
    "        custom_steps (list): List of preprocessing steps in desired order.\n",
    "\n",
    "        Returns:\n",
    "        str: Preprocessed text.\n",
    "        \"\"\"\n",
    "        # Define the default pipeline\n",
    "        default_pipeline = [\n",
    "            \"lowercase\",\n",
    "            \"remove_punctuation\",\n",
    "            \"remove_stopwords\",\n",
    "            \"remove_special_characters\",\n",
    "            \"remove_url\",\n",
    "            \"remove_html_tags\",\n",
    "            \"correct_spellings\",\n",
    "            \"lemmatize_text\",\n",
    "        ]\n",
    "\n",
    "        # Use custom steps if provided, otherwise default steps\n",
    "        steps = custom_steps if custom_steps else default_pipeline\n",
    "        for step in steps:\n",
    "            text = getattr(self, step)(text)\n",
    "        return text\n",
    "\n",
    "    def head(self, texts, n=5):\n",
    "        \"\"\"\n",
    "        Display a summary of the first few entries of the dataset for quick visualization.\n",
    "\n",
    "        Parameters:\n",
    "        texts (list or pd.Series): The dataset or list of text entries to display.\n",
    "        n (int): The number of rows to display. Default is 5.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        if isinstance(texts, (list, pd.Series)):\n",
    "            data = pd.DataFrame({\"Text\": texts[:n]})\n",
    "            data[\"Word Count\"] = data[\"Text\"].apply(lambda x: len(x.split()))\n",
    "            data[\"Character Count\"] = data[\"Text\"].apply(len)\n",
    "            display(data)\n",
    "\n",
    "            # Plotting word counts for quick overview\n",
    "            plt.figure(figsize=(8, 5))\n",
    "            plt.bar(range(n), data[\"Word Count\"], color=\"skyblue\")\n",
    "            plt.xticks(range(n), [f\"Text {i+1}\" for i in range(n)], rotation=45)\n",
    "            plt.xlabel(\"Text Entries\")\n",
    "            plt.ylabel(\"Word Count\")\n",
    "            plt.title(\"Word Count of First Few Text Entries\")\n",
    "            plt.show()\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"The input should be a list or pandas Series of text entries.\"\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After removing punctuation: Hello world This is a test text with HTML html tags and a URL httpsexamplecom\n",
      "Lowercase text: hello, world! this is a test text with html <html> tags and a url: https://example.com.\n",
      "After removing stopwords: Hello, world! This test text HTML <html> tags URL: https://example.com.\n",
      "After removing URLs: Hello, world! This is a test text with HTML <html> tags and a URL: \n",
      "After removing HTML tags: Hello, world! This is a test text with HTML  tags and a URL: https://example.com.\n",
      "After spell correction: This is a test text for spelling correction\n",
      "After lemmatization: run jump play well\n",
      "After stemming: run jump play better\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "processor = TextPreprocessor()\n",
    "\n",
    "# Sample text\n",
    "text = \"Hello, world! This is a test text with HTML <html> tags and a URL: https://example.com.\"\n",
    "\n",
    "# Removing punctuation\n",
    "no_punctuation_text = processor.remove_punctuation(text)\n",
    "print(\"After removing punctuation:\", no_punctuation_text)\n",
    "\n",
    "# Lowercasing text\n",
    "lowercase_text = processor.lowercase(text)\n",
    "print(\"Lowercase text:\", lowercase_text)\n",
    "\n",
    "# Removing stopwords\n",
    "no_stopwords_text = processor.remove_stopwords(text)\n",
    "print(\"After removing stopwords:\", no_stopwords_text)\n",
    "\n",
    "# Removing URLs\n",
    "no_urls_text = processor.remove_url(text)\n",
    "print(\"After removing URLs:\", no_urls_text)\n",
    "\n",
    "# Removing HTML tags\n",
    "no_html_text = processor.remove_html_tags(text)\n",
    "print(\"After removing HTML tags:\", no_html_text)\n",
    "\n",
    "# Correcting spellings\n",
    "corrected_text = processor.correct_spellings(\"This is a tst txt for spellng corection\")\n",
    "print(\"After spell correction:\", corrected_text)\n",
    "\n",
    "# Lemmatizing text\n",
    "lemmatized_text = processor.lemmatize_text(\"running jumped plays better\")\n",
    "print(\"After lemmatization:\", lemmatized_text)\n",
    "\n",
    "\n",
    "# Stemming text\n",
    "stemmed_text = processor.stem_text(\"running jumped plays better\")\n",
    "print(\"After stemming:\", stemmed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'lower'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[129], line 9\u001b[0m\n\u001b[0;32m      4\u001b[0m preprocessor \u001b[38;5;241m=\u001b[39m TextPreprocessor()\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# Example text\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Perform automatic preprocessing\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m processed_text \u001b[38;5;241m=\u001b[39m \u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[38;5;66;03m# Display the preprocessed text\u001b[39;00m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed Text:\u001b[39m\u001b[38;5;124m\"\u001b[39m, processed_text)\n",
      "Cell \u001b[1;32mIn[121], line 88\u001b[0m, in \u001b[0;36mTextPreprocessor.preprocess\u001b[1;34m(self, text, custom_steps)\u001b[0m\n\u001b[0;32m     86\u001b[0m steps \u001b[38;5;241m=\u001b[39m custom_steps \u001b[38;5;28;01mif\u001b[39;00m custom_steps \u001b[38;5;28;01melse\u001b[39;00m default_pipeline\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m steps:\n\u001b[1;32m---> 88\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "Cell \u001b[1;32mIn[121], line 64\u001b[0m, in \u001b[0;36mTextPreprocessor.lowercase\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     63\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mlowercase\u001b[39m(\u001b[38;5;28mself\u001b[39m, text):\n\u001b[1;32m---> 64\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtext\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlower\u001b[49m()\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'lower'"
     ]
    }
   ],
   "source": [
    "# Import the text preprocessing toolkit\n",
    "\n",
    "# Create an instance of the TextPreprocessor class\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Example text\n",
    "\n",
    "# Perform automatic preprocessing\n",
    "processed_text = preprocessor.preprocess(text)\n",
    "\n",
    "# Display the preprocessed text\n",
    "print(\"Processed Text:\", processed_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "sequence item 6: expected str instance, NoneType found",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[131], line 16\u001b[0m\n\u001b[0;32m      7\u001b[0m texts \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNatural Language Processing (NLP) is fascinating! Visit https://nlp.com for more info.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText processing includes <html> tags, URLs like http://example.com, and stopwords.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms clean and prepare our dataset for machine learning models.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     13\u001b[0m ]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Step 1: Preprocess each text in the list\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m processed_texts \u001b[38;5;241m=\u001b[39m [preprocessor\u001b[38;5;241m.\u001b[39mpreprocess(text) \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Step 2: Use the head function to display the first few processed entries\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed Text Entries:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[131], line 16\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      7\u001b[0m texts \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNatural Language Processing (NLP) is fascinating! Visit https://nlp.com for more info.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mText processing includes <html> tags, URLs like http://example.com, and stopwords.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     12\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLet\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124ms clean and prepare our dataset for machine learning models.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     13\u001b[0m ]\n\u001b[0;32m     15\u001b[0m \u001b[38;5;66;03m# Step 1: Preprocess each text in the list\u001b[39;00m\n\u001b[1;32m---> 16\u001b[0m processed_texts \u001b[38;5;241m=\u001b[39m [\u001b[43mpreprocessor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m text \u001b[38;5;129;01min\u001b[39;00m texts]\n\u001b[0;32m     18\u001b[0m \u001b[38;5;66;03m# Step 2: Use the head function to display the first few processed entries\u001b[39;00m\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed Text Entries:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[121], line 88\u001b[0m, in \u001b[0;36mTextPreprocessor.preprocess\u001b[1;34m(self, text, custom_steps)\u001b[0m\n\u001b[0;32m     86\u001b[0m steps \u001b[38;5;241m=\u001b[39m custom_steps \u001b[38;5;28;01mif\u001b[39;00m custom_steps \u001b[38;5;28;01melse\u001b[39;00m default_pipeline\n\u001b[0;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m step \u001b[38;5;129;01min\u001b[39;00m steps:\n\u001b[1;32m---> 88\u001b[0m     text \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstep\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtext\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     89\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m text\n",
      "Cell \u001b[1;32mIn[121], line 61\u001b[0m, in \u001b[0;36mTextPreprocessor.correct_spellings\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     60\u001b[0m         corrected_text\u001b[38;5;241m.\u001b[39mappend(word)\n\u001b[1;32m---> 61\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcorrected_text\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: sequence item 6: expected str instance, NoneType found"
     ]
    }
   ],
   "source": [
    "# Import the text preprocessing toolkit\n",
    "\n",
    "# Create an instance of the TextPreprocessor class\n",
    "preprocessor = TextPreprocessor()\n",
    "\n",
    "# Sample dataset of text entries\n",
    "texts = [\n",
    "    \"Natural Language Processing (NLP) is fascinating! Visit https://nlp.com for more info.\",\n",
    "    \"Text processing includes <html> tags, URLs like http://example.com, and stopwords.\",\n",
    "    \"Punctuation, typos, and other #special characters are common in real-world texts.\",\n",
    "    \"NLP is transforming industries by making sense of large-scale textual data!\",\n",
    "    \"Let's clean and prepare our dataset for machine learning models.\",\n",
    "]\n",
    "\n",
    "# Step 1: Preprocess each text in the list\n",
    "processed_texts = [preprocessor.preprocess(text) for text in texts]\n",
    "\n",
    "# Step 2: Use the head function to display the first few processed entries\n",
    "print(\"Processed Text Entries:\")\n",
    "preprocessor.head(processed_texts, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Gaurav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Gaurav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to\n",
      "[nltk_data]     C:\\Users\\Gaurav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gaurav\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from nltk import pos_tag\n",
    "from spellchecker import SpellChecker\n",
    "import nltk\n",
    "from IPython.display import display\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "# Download required NLTK resources if not already downloaded\n",
    "nltk.download(\"averaged_perceptron_tagger\")\n",
    "nltk.download(\"wordnet\")\n",
    "nltk.download(\"omw-1.4\")\n",
    "nltk.download(\"stopwords\")\n",
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    def __init__(self) -> None:\n",
    "        self.stopwords: set[str] = set(stopwords.words(\"english\"))\n",
    "        self.lemmatizer: WordNetLemmatizer = WordNetLemmatizer()\n",
    "        self.spell: SpellChecker = SpellChecker()\n",
    "        self.ps: PorterStemmer = PorterStemmer()\n",
    "        self.wordnet_map: dict[str, wordnet] = {\n",
    "            \"N\": wordnet.NOUN,\n",
    "            \"V\": wordnet.VERB,\n",
    "            \"J\": wordnet.ADJ,\n",
    "            \"R\": wordnet.ADV,\n",
    "        }\n",
    "\n",
    "    def remove_punctuation(self, text: Optional[str]) -> Optional[str]:\n",
    "        if text:\n",
    "            return text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "        return text\n",
    "\n",
    "    def remove_stopwords(self, text: Optional[str]) -> Optional[str]:\n",
    "        if text:\n",
    "            return \" \".join(\n",
    "                [word for word in text.split() if word not in self.stopwords]\n",
    "            )\n",
    "        return text\n",
    "\n",
    "    def remove_special_characters(self, text: Optional[str]) -> Optional[str]:\n",
    "        if text:\n",
    "            text = re.sub(r\"[^a-zA-Z0-9]\", \" \", text)\n",
    "            text = re.sub(r\"\\s+\", \" \", text)\n",
    "            return text\n",
    "        return text\n",
    "\n",
    "    def stem_text(self, text: Optional[str]) -> Optional[str]:\n",
    "        if text:\n",
    "            return \" \".join([self.ps.stem(word) for word in text.split()])\n",
    "        return text\n",
    "\n",
    "    def lemmatize_text(self, text: Optional[str]) -> Optional[str]:\n",
    "        if text:\n",
    "            pos_text = pos_tag(text.split())\n",
    "            return \" \".join(\n",
    "                [\n",
    "                    self.lemmatizer.lemmatize(\n",
    "                        word, self.wordnet_map.get(pos[0], wordnet.NOUN)\n",
    "                    )\n",
    "                    for word, pos in pos_text\n",
    "                ]\n",
    "            )\n",
    "        return text\n",
    "\n",
    "    def remove_url(self, text: Optional[str]) -> Optional[str]:\n",
    "        if text:\n",
    "            return re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text)\n",
    "        return text\n",
    "\n",
    "    def remove_html_tags(self, text: Optional[str]) -> Optional[str]:\n",
    "        if text:\n",
    "            return re.sub(r\"<[^>]+>\", \"\", text)\n",
    "        return text\n",
    "\n",
    "    def correct_spellings(self, text: Optional[str]) -> Optional[str]:\n",
    "        if text:\n",
    "            corrected_text: List[str] = []\n",
    "            misspelled_words = self.spell.unknown(text.split())\n",
    "            for word in text.split():\n",
    "                if word in misspelled_words:\n",
    "                    corrected_word = self.spell.correction(word)\n",
    "                    corrected_text.append(corrected_word)\n",
    "                else:\n",
    "                    corrected_text.append(word)\n",
    "            return \" \".join(corrected_text)\n",
    "        return text\n",
    "\n",
    "    def lowercase(self, text: Optional[str]) -> Optional[str]:\n",
    "        if text:\n",
    "            return text.lower()\n",
    "        return text\n",
    "\n",
    "    def preprocess(self, text: str, steps: Optional[List[str]] = None) -> str:\n",
    "        \"\"\"\n",
    "        Automatically preprocess text with a default pipeline.\n",
    "        User can specify steps for specific preprocessing order.\n",
    "\n",
    "        Parameters:\n",
    "        text (str): Text to preprocess.\n",
    "        steps (list): List of preprocessing steps in desired order.\n",
    "\n",
    "        Returns:\n",
    "        str: Preprocessed text.\n",
    "        \"\"\"\n",
    "        default_pipeline = [\n",
    "            \"lowercase\",\n",
    "            \"remove_punctuation\",\n",
    "            \"remove_stopwords\",\n",
    "            \"remove_special_characters\",\n",
    "            \"remove_url\",\n",
    "            \"remove_html_tags\",\n",
    "            \"correct_spellings\",\n",
    "            \"lemmatize_text\",\n",
    "        ]\n",
    "        steps = steps if steps else default_pipeline\n",
    "        for step in steps:\n",
    "            text = getattr(self, step)(text)  # type: ignore\n",
    "        return text\n",
    "\n",
    "    def head(self, texts: Union[List[str], pd.Series], n: int = 5) -> None:\n",
    "        \"\"\"\n",
    "        Display a summary of the first few entries of the dataset for quick visualization.\n",
    "\n",
    "        Parameters:\n",
    "        texts (list or pd.Series): The dataset or list of text entries to display.\n",
    "        n (int): The number of rows to display. Default is 5.\n",
    "\n",
    "        Returns:\n",
    "        None\n",
    "        \"\"\"\n",
    "        if isinstance(texts, (list, pd.Series)):\n",
    "            data = pd.DataFrame({\"Text\": texts[:n]})\n",
    "            data[\"Word Count\"] = data[\"Text\"].apply(lambda x: len(x.split()))\n",
    "            data[\"Character Count\"] = data[\"Text\"].apply(len)\n",
    "            display(data)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    TextPreprocessor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Text</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Character Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>sample sentence punctuation</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>check website httpsexamplecom information</td>\n",
       "      <td>4</td>\n",
       "      <td>41</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>pthis paragraph html tagsp</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>im test spellng correction</td>\n",
       "      <td>4</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>another example sentence test preprocessing step</td>\n",
       "      <td>6</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               Text  Word Count  \\\n",
       "0                       sample sentence punctuation           3   \n",
       "1         check website httpsexamplecom information           4   \n",
       "2                        pthis paragraph html tagsp           4   \n",
       "3                        im test spellng correction           4   \n",
       "4  another example sentence test preprocessing step           6   \n",
       "\n",
       "   Character Count  \n",
       "0               27  \n",
       "1               41  \n",
       "2               26  \n",
       "3               26  \n",
       "4               48  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Import the TextPreprocessor class\n",
    "\n",
    "# Initialize the TextPreprocessor\n",
    "processor = TextPreprocessor()\n",
    "\n",
    "# Sample text data\n",
    "texts = [\n",
    "    \"This is a sample sentence, with punctuation!\",\n",
    "    \"Check out the website: https://example.com for more information.\",\n",
    "    \"<p>This is a paragraph with HTML tags.</p>\",\n",
    "    \"I'm here to test spellng corrections.\",\n",
    "    \"Another example sentence for testing preprocessing steps.\",\n",
    "]\n",
    "\n",
    "# Apply preprocessing with a custom pipeline\n",
    "cleaned_texts = [\n",
    "    processor.preprocess(\n",
    "        text,\n",
    "        steps=[\n",
    "            \"lowercase\",\n",
    "            \"remove_punctuation\",\n",
    "            \"remove_stopwords\",\n",
    "            \"lemmatize_text\",\n",
    "            \"remove_special_characters\",\n",
    "            \"remove_url\",\n",
    "            \"remove_html_tags\",\n",
    "        ],\n",
    "    )\n",
    "    for text in texts\n",
    "]\n",
    "\n",
    "# Display the first few cleaned texts\n",
    "processor.head(cleaned_texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
