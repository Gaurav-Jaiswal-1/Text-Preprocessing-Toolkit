{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 14:47:16,805 - INFO - Step 'lowercase' completed.\n",
      "2024-12-08 14:47:16,806 - INFO - Step 'remove_punctuation' completed.\n",
      "2024-12-08 14:47:16,807 - INFO - Step 'remove_special_characters' completed.\n",
      "2024-12-08 14:47:16,809 - INFO - Step 'remove_url' completed.\n",
      "2024-12-08 14:47:16,809 - INFO - Step 'remove_html_tags' completed.\n",
      "2024-12-08 14:47:16,810 - INFO - Step 'correct_spellings' completed.\n",
      "2024-12-08 14:47:16,831 - INFO - Step 'lemmatize_text' completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th be an exampel of automated text preprocessing\n"
     ]
    }
   ],
   "source": [
    "from tptk import TextPreprocessor\n",
    "\n",
    "tp = TextPreprocessor()\n",
    "text = \"Ths is an exampel of automated text preprocessing!!\"\n",
    "\n",
    "# Apply a preprocessing pipeline\n",
    "processed_text = tp.preprocess(\n",
    "    text,\n",
    "    steps=[\n",
    "        \"lowercase\",\n",
    "        \"remove_punctuation\",\n",
    "        \"remove_special_characters\",\n",
    "        \"remove_url\",\n",
    "        \"remove_html_tags\",\n",
    "        \"correct_spellings\",\n",
    "        \"lemmatize_text\"\n",
    "    ]\n",
    ")\n",
    "\n",
    "print(processed_text)\n",
    "# Output: ['This', 'example', 'automated', 'text', 'preprocess']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Ths', 'is', 'an', 'exampel', 'of', 'automated', 'text', 'preprocessing', '!', '!']\n"
     ]
    }
   ],
   "source": [
    "text2 = \"Ths is an exampel of automated text preprocessing!!\"\n",
    "\n",
    "tokens = tp.tokenize(text2)\n",
    "print(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 12:26:37,830 - INFO - Step 'lowercase' completed.\n",
      "2024-12-08 12:26:37,831 - INFO - Step 'remove_url' completed.\n",
      "2024-12-08 12:26:37,833 - INFO - Step 'remove_html_tags' completed.\n",
      "2024-12-08 12:26:37,834 - INFO - Step 'remove_punctuation' completed.\n",
      "2024-12-08 12:26:37,837 - INFO - Step 'remove_special_characters' completed.\n",
      "2024-12-08 12:26:37,838 - INFO - Step 'correct_spellings' completed.\n",
      "2024-12-08 12:26:37,840 - INFO - Step 'lemmatize_text' completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "check out this link and html tag\n"
     ]
    }
   ],
   "source": [
    "text3 = \"Check out this link: https://example.com and <b>HTML tags!</b>\"\n",
    "processed_text = tp.preprocess(text)\n",
    "print(processed_text)\n",
    "# Output: \"check out this link and html tags\"\n",
    "\n",
    "tp.head(processed_text, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 12:30:55,731 - INFO - Step 'lowercase' completed.\n",
      "2024-12-08 12:30:55,733 - INFO - Step 'remove_url' completed.\n",
      "2024-12-08 12:30:55,734 - INFO - Step 'remove_html_tags' completed.\n",
      "2024-12-08 12:30:55,736 - INFO - Step 'remove_punctuation' completed.\n",
      "2024-12-08 12:30:55,737 - INFO - Step 'remove_special_characters' completed.\n",
      "2024-12-08 12:30:55,738 - INFO - Step 'correct_spellings' completed.\n",
      "2024-12-08 12:30:55,740 - INFO - Step 'lemmatize_text' completed.\n",
      "2024-12-08 12:30:55,741 - INFO - Step 'lowercase' completed.\n",
      "2024-12-08 12:30:55,742 - INFO - Step 'remove_url' completed.\n",
      "2024-12-08 12:30:55,743 - INFO - Step 'remove_html_tags' completed.\n",
      "2024-12-08 12:30:55,744 - INFO - Step 'remove_punctuation' completed.\n",
      "2024-12-08 12:30:55,745 - INFO - Step 'remove_special_characters' completed.\n",
      "2024-12-08 12:30:55,746 - INFO - Step 'correct_spellings' completed.\n",
      "2024-12-08 12:30:55,747 - INFO - Step 'lemmatize_text' completed.\n",
      "2024-12-08 12:30:55,748 - INFO - Step 'lowercase' completed.\n",
      "2024-12-08 12:30:55,750 - INFO - Step 'remove_url' completed.\n",
      "2024-12-08 12:30:55,751 - INFO - Step 'remove_html_tags' completed.\n",
      "2024-12-08 12:30:55,753 - INFO - Step 'remove_punctuation' completed.\n",
      "2024-12-08 12:30:55,755 - INFO - Step 'remove_special_characters' completed.\n",
      "2024-12-08 12:30:55,756 - INFO - Step 'correct_spellings' completed.\n",
      "2024-12-08 12:30:55,759 - INFO - Step 'lemmatize_text' completed.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Original Text</th>\n",
       "      <th>Processed Text</th>\n",
       "      <th>Word Count</th>\n",
       "      <th>Character Count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ths is the frst example.</td>\n",
       "      <td>th be the frst example</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Preprocessing is &lt;b&gt;important&lt;/b&gt;!</td>\n",
       "      <td>preprocessing be important</td>\n",
       "      <td>3</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Visit https://example.com for details.</td>\n",
       "      <td>visit for detail</td>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                            Original Text              Processed Text  \\\n",
       "0                Ths is the frst example.      th be the frst example   \n",
       "1      Preprocessing is <b>important</b>!  preprocessing be important   \n",
       "2  Visit https://example.com for details.            visit for detail   \n",
       "\n",
       "   Word Count  Character Count  \n",
       "0           5               22  \n",
       "1           3               26  \n",
       "2           3               16  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "texts = [\n",
    "    \"Ths is the frst example.\",\n",
    "    \"Preprocessing is <b>important</b>!\",\n",
    "    \"Visit https://example.com for details.\"\n",
    "]\n",
    "\n",
    "tp.head(texts, n=3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 14:47:02,235 - INFO - Step 'lowercase' completed.\n",
      "2024-12-08 14:47:02,237 - INFO - Step 'remove_url' completed.\n",
      "2024-12-08 14:47:02,238 - INFO - Step 'remove_html_tags' completed.\n",
      "2024-12-08 14:47:02,238 - INFO - Step 'remove_punctuation' completed.\n",
      "2024-12-08 14:47:02,240 - INFO - Step 'correct_spellings' completed.\n",
      "2024-12-08 14:47:02,241 - INFO - Step 'lemmatize_text' completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "th be an example of text preprocessing visit\n"
     ]
    }
   ],
   "source": [
    "text4 = \"Thsi is an <b>example</b> of text preprocessing! Visit https://example.com\"\n",
    "\n",
    "# Apply a preprocessing pipeline\n",
    "processed_text = tp.preprocess(\n",
    "    text, steps=[\n",
    "        \"lowercase\",\n",
    "        \"remove_url\",\n",
    "        \"remove_html_tags\",\n",
    "        \"remove_punctuation\",\n",
    "        \"correct_spellings\",\n",
    "        \"lemmatize_text\"\n",
    "    ]\n",
    ")\n",
    "print(processed_text)\n",
    "# Output: \"this example text preprocess\"\n",
    "tp.head(processed_text, n=10)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import string\n",
    "# import re\n",
    "# import pandas as pd\n",
    "# from collections import Counter\n",
    "# from nltk.corpus import stopwords, wordnet\n",
    "# from nltk.stem import WordNetLemmatizer\n",
    "# from nltk import pos_tag, word_tokenize\n",
    "# from spellchecker import SpellChecker\n",
    "# from typing import List, Optional, Union\n",
    "# from IPython.display import display\n",
    "# import logging\n",
    "\n",
    "# # Download required NLTK resources\n",
    "# import nltk\n",
    "# nltk.download(\"averaged_perceptron_tagger_eng\", quiet=True)\n",
    "# nltk.download(\"wordnet\", quiet=True)\n",
    "# nltk.download(\"omw-1.4\", quiet=True)\n",
    "# nltk.download(\"stopwords\", quiet=True)\n",
    "# nltk.download(\"punkt_tab\", quiet=True)\n",
    "\n",
    "# # Configure logging\n",
    "# # logging.basicConfig(level=logging.INFO, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "# class TextPreprocessor:\n",
    "#     \"\"\"\n",
    "#     Comprehensive text preprocessing class for NLP tasks.\n",
    "#     \"\"\"\n",
    "#     __version__ = \"0.0.9\"  # Updated version with optimized code\n",
    "\n",
    "#     def __init__(self, custom_stopwords: Optional[List[str]] = None) -> None:\n",
    "#         \"\"\"\n",
    "#         Initialize the text preprocessor.\n",
    "\n",
    "#         Parameters:\n",
    "#         custom_stopwords (list, optional): List of additional stopwords to remove.\n",
    "#         \"\"\"\n",
    "#         self.stopwords = set(stopwords.words(\"english\"))\n",
    "#         if custom_stopwords:\n",
    "#             self.stopwords.update(custom_stopwords)\n",
    "\n",
    "#         self.lemmatizer = WordNetLemmatizer()\n",
    "#         self.spell_checker = SpellChecker()\n",
    "#         self.wordnet_map = {\n",
    "#             \"N\": wordnet.NOUN,\n",
    "#             \"V\": wordnet.VERB,\n",
    "#             \"J\": wordnet.ADJ,\n",
    "#             \"R\": wordnet.ADV,\n",
    "#         }\n",
    "\n",
    "#     def tokenize(self, text: Optional[str]) -> List[str]:\n",
    "#         \"\"\"Tokenize text into words.\"\"\"\n",
    "#         return word_tokenize(text) if text else []\n",
    "\n",
    "#     def remove_punctuation(self, text: Optional[str]) -> Optional[str]:\n",
    "#         \"\"\"Remove punctuation from text.\"\"\"\n",
    "#         return re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text) if text else text\n",
    "\n",
    "#     def remove_stopwords(self, tokens: List[str]) -> List[str]:\n",
    "#         \"\"\"Remove stopwords from tokenized text.\"\"\"\n",
    "#         return [word for word in tokens if word.lower() not in self.stopwords]\n",
    "\n",
    "#     def remove_special_characters(self, text: Optional[str]) -> Optional[str]:\n",
    "#         \"\"\"Remove special characters from text.\"\"\"\n",
    "#         return re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text).strip() if text else text\n",
    "\n",
    "#     def lemmatize_text(self, text: Optional[str]) -> Optional[str]:\n",
    "#         \"\"\"Lemmatize text using WordNet.\"\"\"\n",
    "#         if not text:\n",
    "#             return text\n",
    "#         tokens = self.tokenize(text)\n",
    "#         pos_tags = pos_tag(tokens)\n",
    "#         return \" \".join(\n",
    "#             self.lemmatizer.lemmatize(word, self.wordnet_map.get(pos[0].upper(), wordnet.NOUN))\n",
    "#             for word, pos in pos_tags\n",
    "#         )\n",
    "\n",
    "#     def correct_spellings(self, text: Optional[str]) -> Optional[str]:\n",
    "#         \"\"\"Correct misspelled words in text.\"\"\"\n",
    "#         if not text:\n",
    "#             return text\n",
    "#         tokens = self.tokenize(text)\n",
    "#         return \" \".join(self.spell_checker.correction(word) if word in self.spell_checker else word for word in tokens)\n",
    "\n",
    "#     def lowercase(self, text: Optional[str]) -> Optional[str]:\n",
    "#         \"\"\"Convert text to lowercase.\"\"\"\n",
    "#         return text.lower() if text else text\n",
    "\n",
    "#     def remove_url(self, text: Optional[str]) -> Optional[str]:\n",
    "#         \"\"\"Remove URLs from text.\"\"\"\n",
    "#         return re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text) if text else text\n",
    "\n",
    "#     def remove_html_tags(self, text: Optional[str]) -> Optional[str]:\n",
    "#         \"\"\"Remove HTML tags from text.\"\"\"\n",
    "#         return re.sub(r\"<[^>]+>\", \"\", text) if text else text\n",
    "\n",
    "#     def preprocess(\n",
    "#         self, text: str, steps: Optional[List[str]] = None\n",
    "#     ) -> Optional[str]:\n",
    "#         \"\"\"\n",
    "#         Preprocess text with a pipeline of processing steps.\n",
    "\n",
    "#         Parameters:\n",
    "#         text (str): The input text to preprocess.\n",
    "#         steps (list, optional): List of preprocessing steps in desired order.\n",
    "\n",
    "#         Returns:\n",
    "#         str: Preprocessed text.\n",
    "#         \"\"\"\n",
    "#         if not text:\n",
    "#             return text\n",
    "\n",
    "#         steps = steps or [\n",
    "#             \"lowercase\",\n",
    "#             \"remove_url\",\n",
    "#             \"remove_html_tags\",\n",
    "#             \"remove_punctuation\",\n",
    "#             \"remove_special_characters\",\n",
    "#             \"correct_spellings\",\n",
    "#             \"lemmatize_text\",\n",
    "#         ]\n",
    "\n",
    "#         for step in steps:\n",
    "#             try:\n",
    "#                 func = getattr(self, step)\n",
    "#                 text = func(text)\n",
    "#                 logging.info(f\"Step '{step}' completed.\")\n",
    "#             except Exception as e:\n",
    "#                 logging.error(f\"Error during step '{step}': {e}\")\n",
    "#         return text\n",
    "\n",
    "#     def head(self, texts: Union[List[str], pd.Series], n: int = 5) -> None:\n",
    "#         \"\"\"\n",
    "#         Display a summary of the first few entries of the dataset.\n",
    "\n",
    "#         Parameters:\n",
    "#         texts (list or pd.Series): The dataset or list of text entries.\n",
    "#         n (int): Number of rows to display.\n",
    "#         \"\"\"\n",
    "#         if isinstance(texts, (list, pd.Series)):\n",
    "#             data = pd.DataFrame({\"Original Text\": texts[:n]})\n",
    "#             data[\"Processed Text\"] = data[\"Original Text\"].apply(self.preprocess)\n",
    "#             data[\"Word Count\"] = data[\"Processed Text\"].apply(lambda x: len(x.split()) if x else 0)\n",
    "#             data[\"Character Count\"] = data[\"Processed Text\"].apply(len)\n",
    "#             display(data)\n",
    "\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "#     # Example usage\n",
    "#     tpt = TextPreprocessor()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 14:49:32,567 - ERROR - Error during step 'lowercase': 'list' object has no attribute 'lower'\n",
      "2024-12-08 14:49:32,569 - ERROR - Error during step 'remove_url': expected string or bytes-like object\n",
      "2024-12-08 14:49:32,570 - ERROR - Error during step 'remove_html_tags': expected string or bytes-like object\n",
      "2024-12-08 14:49:32,570 - ERROR - Error during step 'remove_punctuation': expected string or bytes-like object\n",
      "2024-12-08 14:49:32,571 - ERROR - Error during step 'correct_spellings': expected string or bytes-like object\n",
      "2024-12-08 14:49:32,572 - ERROR - Error during step 'lemmatize_text': expected string or bytes-like object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human language. It enables machines to understand, interpret, and generate human language in a valuable way.', 'Machine learning is a subset of artificial intelligence that focuses on building systems that can learn from data and improve their performance over time. It involves algorithms that can identify patterns and make predictions.', 'Text preprocessing is a crucial step in NLP that involves cleaning and preparing raw text data for further analysis. It typically includes tasks such as removing noise, stemming, lemmatization, and converting text to a standard format.', 'Deep learning models, especially neural networks, have revolutionized many fields like computer vision and natural language processing. They are capable of learning complex patterns in large datasets and making highly accurate predictions.', 'Data science combines various fields like statistics, computer science, and domain knowledge to extract insights from structured and unstructured data. It uses tools like machine learning and big data technologies to solve complex problems.', 'Sentiment analysis is the process of determining the emotional tone behind a body of text. It helps in understanding the sentiment expressed in social media posts, reviews, or any other form of text data.', 'Text classification is the task of categorizing text into predefined groups or categories. It has applications in spam detection, sentiment analysis, and topic categorization, among others.', 'Information retrieval involves obtaining information from a large repository of data. It is widely used in search engines, recommendation systems, and document retrieval systems.']\n"
     ]
    }
   ],
   "source": [
    "text = [\n",
    "    \"Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human language. It enables machines to understand, interpret, and generate human language in a valuable way.\",\n",
    "    \"Machine learning is a subset of artificial intelligence that focuses on building systems that can learn from data and improve their performance over time. It involves algorithms that can identify patterns and make predictions.\",\n",
    "    \"Text preprocessing is a crucial step in NLP that involves cleaning and preparing raw text data for further analysis. It typically includes tasks such as removing noise, stemming, lemmatization, and converting text to a standard format.\",\n",
    "    \"Deep learning models, especially neural networks, have revolutionized many fields like computer vision and natural language processing. They are capable of learning complex patterns in large datasets and making highly accurate predictions.\",\n",
    "    \"Data science combines various fields like statistics, computer science, and domain knowledge to extract insights from structured and unstructured data. It uses tools like machine learning and big data technologies to solve complex problems.\",\n",
    "    \"Sentiment analysis is the process of determining the emotional tone behind a body of text. It helps in understanding the sentiment expressed in social media posts, reviews, or any other form of text data.\",\n",
    "    \"Text classification is the task of categorizing text into predefined groups or categories. It has applications in spam detection, sentiment analysis, and topic categorization, among others.\",\n",
    "    \"Information retrieval involves obtaining information from a large repository of data. It is widely used in search engines, recommendation systems, and document retrieval systems.\"\n",
    "    ]\n",
    "\n",
    "# Apply a preprocessing pipeline\n",
    "processed_text = tp.preprocess(\n",
    "    text, steps=[\n",
    "        \"lowercase\",\n",
    "        \"remove_url\",\n",
    "        \"remove_html_tags\",\n",
    "        \"remove_punctuation\",\n",
    "        \"correct_spellings\",\n",
    "        \"lemmatize_text\"\n",
    "    ]\n",
    ")\n",
    "print(processed_text)\n",
    "# Output: \"this example text preprocess\"\n",
    "# tp.head(text, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "import re\n",
    "import pandas as pd\n",
    "from collections import Counter\n",
    "from nltk.corpus import stopwords, wordnet\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk import pos_tag, word_tokenize\n",
    "from spellchecker import SpellChecker\n",
    "from typing import List, Optional, Union\n",
    "from IPython.display import display\n",
    "import logging\n",
    "\n",
    "# Download required NLTK resources\n",
    "import nltk\n",
    "nltk.download(\"averaged_perceptron_tagger\", quiet=True)\n",
    "nltk.download(\"wordnet\", quiet=True)\n",
    "nltk.download(\"omw-1.4\", quiet=True)\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "nltk.download(\"punkt\", quiet=True)\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.ERROR, format=\"%(asctime)s - %(levelname)s - %(message)s\")\n",
    "\n",
    "\n",
    "class TextPreprocessor:\n",
    "    \"\"\"\n",
    "    Comprehensive text preprocessing class for NLP tasks.\n",
    "    \"\"\"\n",
    "    __version__ = \"1.0.0\"\n",
    "\n",
    "    def __init__(self, custom_stopwords: Optional[List[str]] = None) -> None:\n",
    "        \"\"\"\n",
    "        Initialize the text preprocessor.\n",
    "\n",
    "        Parameters:\n",
    "        custom_stopwords (list, optional): List of additional stopwords to remove.\n",
    "        \"\"\"\n",
    "        self.stopwords = set(stopwords.words(\"english\"))\n",
    "        if custom_stopwords:\n",
    "            self.stopwords.update(custom_stopwords)\n",
    "\n",
    "        self.lemmatizer = WordNetLemmatizer()\n",
    "        self.spell_checker = SpellChecker()\n",
    "        self.wordnet_map = {\n",
    "            \"N\": wordnet.NOUN,\n",
    "            \"V\": wordnet.VERB,\n",
    "            \"J\": wordnet.ADJ,\n",
    "            \"R\": wordnet.ADV,\n",
    "        }\n",
    "\n",
    "    def tokenize(self, text: Optional[str]) -> List[str]:\n",
    "        \"\"\"Tokenize text into words.\"\"\"\n",
    "        return word_tokenize(text) if text else []\n",
    "\n",
    "    def remove_punctuation(self, text: Optional[str]) -> Optional[str]:\n",
    "        \"\"\"Remove punctuation from text.\"\"\"\n",
    "        return re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text) if text else text\n",
    "\n",
    "    def remove_stopwords(self, tokens: List[str]) -> List[str]:\n",
    "        \"\"\"Remove stopwords from tokenized text.\"\"\"\n",
    "        return [word for word in tokens if word.lower() not in self.stopwords]\n",
    "\n",
    "    def remove_special_characters(self, text: Optional[str]) -> Optional[str]:\n",
    "        \"\"\"Remove special characters from text.\"\"\"\n",
    "        return re.sub(r\"[^a-zA-Z0-9\\s]\", \"\", text).strip() if text else text\n",
    "\n",
    "    def lemmatize_text(self, text: Optional[str]) -> Optional[str]:\n",
    "        \"\"\"Lemmatize text using WordNet.\"\"\"\n",
    "        if not text:\n",
    "            return text\n",
    "        tokens = self.tokenize(text)\n",
    "        pos_tags = pos_tag(tokens)\n",
    "        return \" \".join(\n",
    "            self.lemmatizer.lemmatize(word, self.wordnet_map.get(pos[0].upper(), wordnet.NOUN))\n",
    "            for word, pos in pos_tags\n",
    "        )\n",
    "\n",
    "    def correct_spellings(self, text: Optional[str]) -> Optional[str]:\n",
    "        \"\"\"Correct misspelled words in text.\"\"\"\n",
    "        if not text:\n",
    "            return text\n",
    "        tokens = self.tokenize(text)\n",
    "        return \" \".join(self.spell_checker.correction(word) if word in self.spell_checker else word for word in tokens)\n",
    "\n",
    "    def lowercase(self, text: Optional[str]) -> Optional[str]:\n",
    "        \"\"\"Convert text to lowercase.\"\"\"\n",
    "        return text.lower() if text else text\n",
    "\n",
    "    def remove_url(self, text: Optional[str]) -> Optional[str]:\n",
    "        \"\"\"Remove URLs from text.\"\"\"\n",
    "        return re.sub(r\"https?://\\S+|www\\.\\S+\", \"\", text) if text else text\n",
    "\n",
    "    def remove_html_tags(self, text: Optional[str]) -> Optional[str]:\n",
    "        \"\"\"Remove HTML tags from text.\"\"\"\n",
    "        return re.sub(r\"<[^>]+>\", \"\", text) if text else text\n",
    "\n",
    "    def preprocess(\n",
    "        self, text: Optional[Union[str, float, int]], steps: Optional[List[str]] = None\n",
    "    ) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Preprocess text with a pipeline of processing steps.\n",
    "\n",
    "        Parameters:\n",
    "        text (str): The input text to preprocess.\n",
    "        steps (list, optional): List of preprocessing steps in desired order.\n",
    "\n",
    "        Returns:\n",
    "        str: Preprocessed text.\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str):\n",
    "            logging.error(f\"Input is not a string: {text}\")\n",
    "            return None\n",
    "        if not text:\n",
    "            return text\n",
    "\n",
    "        steps = steps or [\n",
    "            \"lowercase\",\n",
    "            \"remove_url\",\n",
    "            \"remove_html_tags\",\n",
    "            \"remove_punctuation\",\n",
    "            \"remove_special_characters\",\n",
    "            \"correct_spellings\",\n",
    "            \"lemmatize_text\",\n",
    "        ]\n",
    "\n",
    "        for step in steps:\n",
    "            try:\n",
    "                func = getattr(self, step)\n",
    "                text = func(text)\n",
    "            except Exception as e:\n",
    "                logging.error(f\"Error during step '{step}': {e}\")\n",
    "        return text\n",
    "\n",
    "    def preprocess_dataset(self, texts: Union[List[str], pd.Series], n: int = 5) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Preprocess a dataset of text entries.\n",
    "\n",
    "        Parameters:\n",
    "        texts (list or pd.Series): The dataset or list of text entries.\n",
    "        n (int): Number of rows to display.\n",
    "\n",
    "        Returns:\n",
    "        pd.DataFrame: DataFrame with processed text and statistics.\n",
    "        \"\"\"\n",
    "        if isinstance(texts, (list, pd.Series)):\n",
    "            data = pd.DataFrame({\"Original Text\": texts})\n",
    "            data[\"Processed Text\"] = data[\"Original Text\"].apply(self.preprocess)\n",
    "            data[\"Word Count\"] = data[\"Processed Text\"].apply(lambda x: len(x.split()) if x else 0)\n",
    "            data[\"Character Count\"] = data[\"Processed Text\"].apply(len)\n",
    "            display(data.head(n))\n",
    "            return data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    tpt = TextPreprocessor()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 18:19:02,308 - ERROR - Error during step 'lowercase': 'list' object has no attribute 'lower'\n",
      "2024-12-08 18:19:02,309 - ERROR - Error during step 'remove_url': expected string or bytes-like object\n",
      "2024-12-08 18:19:02,311 - ERROR - Error during step 'remove_html_tags': expected string or bytes-like object\n",
      "2024-12-08 18:19:02,312 - ERROR - Error during step 'remove_punctuation': expected string or bytes-like object\n",
      "2024-12-08 18:19:02,313 - ERROR - Error during step 'correct_spellings': expected string or bytes-like object\n",
      "2024-12-08 18:19:02,314 - ERROR - Error during step 'lemmatize_text': expected string or bytes-like object\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human language. It enables machines to understand, interpret, and generate human language in a valuable way.', 'Machine learning is a subset of artificial intelligence that focuses on building systems that can learn from data and improve their performance over time. It involves algorithms that can identify patterns and make predictions.', 'Text preprocessing is a crucial step in NLP that involves cleaning and preparing raw text data for further analysis. It typically includes tasks such as removing noise, stemming, lemmatization, and converting text to a standard format.', 'Deep learning models, especially neural networks, have revolutionized many fields like computer vision and natural language processing. They are capable of learning complex patterns in large datasets and making highly accurate predictions.', 'Data science combines various fields like statistics, computer science, and domain knowledge to extract insights from structured and unstructured data. It uses tools like machine learning and big data technologies to solve complex problems.', 'Sentiment analysis is the process of determining the emotional tone behind a body of text. It helps in understanding the sentiment expressed in social media posts, reviews, or any other form of text data.', 'Text classification is the task of categorizing text into predefined groups or categories. It has applications in spam detection, sentiment analysis, and topic categorization, among others.', 'Information retrieval involves obtaining information from a large repository of data. It is widely used in search engines, recommendation systems, and document retrieval systems.']\n"
     ]
    }
   ],
   "source": [
    "text = [\n",
    "    \"Natural language processing (NLP) is a field of artificial intelligence that focuses on the interaction between computers and human language. It enables machines to understand, interpret, and generate human language in a valuable way.\",\n",
    "    \"Machine learning is a subset of artificial intelligence that focuses on building systems that can learn from data and improve their performance over time. It involves algorithms that can identify patterns and make predictions.\",\n",
    "    \"Text preprocessing is a crucial step in NLP that involves cleaning and preparing raw text data for further analysis. It typically includes tasks such as removing noise, stemming, lemmatization, and converting text to a standard format.\",\n",
    "    \"Deep learning models, especially neural networks, have revolutionized many fields like computer vision and natural language processing. They are capable of learning complex patterns in large datasets and making highly accurate predictions.\",\n",
    "    \"Data science combines various fields like statistics, computer science, and domain knowledge to extract insights from structured and unstructured data. It uses tools like machine learning and big data technologies to solve complex problems.\",\n",
    "    \"Sentiment analysis is the process of determining the emotional tone behind a body of text. It helps in understanding the sentiment expressed in social media posts, reviews, or any other form of text data.\",\n",
    "    \"Text classification is the task of categorizing text into predefined groups or categories. It has applications in spam detection, sentiment analysis, and topic categorization, among others.\",\n",
    "    \"Information retrieval involves obtaining information from a large repository of data. It is widely used in search engines, recommendation systems, and document retrieval systems.\"\n",
    "    ]\n",
    "\n",
    "# Apply a preprocessing pipeline\n",
    "processed_text = tp.preprocess(\n",
    "    text, steps=[\n",
    "        \"lowercase\",\n",
    "        \"remove_url\",\n",
    "        \"remove_html_tags\",\n",
    "        \"remove_punctuation\",\n",
    "        \"correct_spellings\",\n",
    "        \"lemmatize_text\"\n",
    "    ]\n",
    ")\n",
    "print(processed_text)\n",
    "# Output: \"this example text preprocess\"\n",
    "# tp.head(text, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-08 18:34:54,187 - ERROR - Invalid input type. Expected string, got <class 'NoneType'>.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[29], line 40\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Dataset example\u001b[39;00m\n\u001b[0;32m     33\u001b[0m dataset \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mSeries([\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThis is the first entry.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnother example with <b>HTML tags</b>!\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m12345 is just a number.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     39\u001b[0m ])\n\u001b[1;32m---> 40\u001b[0m processed_data \u001b[38;5;241m=\u001b[39m \u001b[43mtpt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpreprocess_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;28mprint\u001b[39m(processed_data)\n",
      "Cell \u001b[1;32mIn[28], line 146\u001b[0m, in \u001b[0;36mTextPreprocessor.preprocess_dataset\u001b[1;34m(self, texts, n)\u001b[0m\n\u001b[0;32m    144\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed Text\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOriginal Text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpreprocess)\n\u001b[0;32m    145\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWord Count\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed Text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mlen\u001b[39m(x\u001b[38;5;241m.\u001b[39msplit()) \u001b[38;5;28;01mif\u001b[39;00m x \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m--> 146\u001b[0m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCharacter Count\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m data[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mProcessed Text\u001b[39m\u001b[38;5;124m\"\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28mlen\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mx\u001b[49m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m    147\u001b[0m display(data\u001b[38;5;241m.\u001b[39mhead(n))\n\u001b[0;32m    148\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "\u001b[1;31mNameError\u001b[0m: name 'x' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import List, Optional, Union\n",
    "\n",
    "def preprocess_dataset(self, texts: Union[List[str], pd.Series], n: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Preprocess a dataset of text entries.\n",
    "\n",
    "    Parameters:\n",
    "    texts (list or pd.Series): The dataset or list of text entries.\n",
    "    n (int): Number of rows to display.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: DataFrame with processed text and statistics.\n",
    "    \"\"\"\n",
    "    if not isinstance(texts, (list, pd.Series)):\n",
    "        logging.error(f\"Invalid input type. Expected list or pandas Series, got {type(texts)}.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    data = pd.DataFrame({\"Original Text\": texts})\n",
    "    data[\"Processed Text\"] = data[\"Original Text\"].apply(self.preprocess)\n",
    "    data[\"Word Count\"] = data[\"Processed Text\"].apply(lambda x: len(x.split()) if isinstance(x, str) else 0)\n",
    "    data[\"Character Count\"] = data[\"Processed Text\"].apply(lambda x: len(x) if isinstance(x, str) else 0)\n",
    "    display(data.head(n))\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    tpt = TextPreprocessor()\n",
    "\n",
    "    # Dataset example\n",
    "    dataset = pd.Series([\n",
    "        \"This is the first entry.\",\n",
    "        \"Another example with <b>HTML tags</b>!\",\n",
    "        \"Visit our site: https://example.com\",\n",
    "        None,\n",
    "        \"12345 is just a number.\"\n",
    "    ])\n",
    "    processed_data = tpt.preprocess_dataset(dataset)\n",
    "    print(processed_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
